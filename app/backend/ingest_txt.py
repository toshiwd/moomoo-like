import os
import json
import re
import time
from datetime import datetime, timezone

import pandas as pd

from db import get_conn, init_schema


REPO_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
DEFAULT_PAN_CODE_PATH = os.path.join(REPO_ROOT, "tools", "code.txt")
DEFAULT_PAN_OUT_DIR = os.path.join(REPO_ROOT, "data", "txt")


def resolve_data_dir() -> str:
    env = os.getenv("PAN_OUT_TXT_DIR") or os.getenv("TXT_DATA_DIR")
    if env:
        return os.path.abspath(env)
    return os.path.abspath(DEFAULT_PAN_OUT_DIR)


DATA_DIR = resolve_data_dir()
CODE_PATTERN_DEFAULT = r"^[0-9A-Za-z]{4,16}$"
CODE_PATTERN = re.compile(os.getenv("CODE_PATTERN", CODE_PATTERN_DEFAULT))
STRICT_CODE_VALIDATION = os.getenv("CODE_STRICT", "0") == "1"
USE_CODE_TXT = os.getenv("USE_CODE_TXT", "0") == "1"


def find_code_txt_path(data_dir: str) -> str | None:
    code_path = os.path.abspath(os.getenv("PAN_CODE_TXT_PATH") or DEFAULT_PAN_CODE_PATH)
    if os.path.exists(code_path):
        return code_path
    return None


def name_from_filename(path: str, code: str) -> str | None:
    base = os.path.splitext(os.path.basename(path))[0]
    if "_" not in base:
        return None
    code_part, name_part = base.split("_", 1)
    if code_part != code:
        return None
    name = name_part.strip()
    return name if name else None



def _build_ma_series(values: list[float], period: int) -> list[float | None]:
    if period <= 0:
        return [None for _ in values]
    result: list[float | None] = []
    total = 0.0
    for index, value in enumerate(values):
        total += value
        if index >= period:
            total -= values[index - period]
        if index >= period - 1:
            result.append(total / period)
        else:
            result.append(None)
    return result


def _count_streak(values: list[float], averages: list[float | None], direction: str) -> int | None:
    count = 0
    opposite = 0
    has_values = False
    for value, avg in zip(values, averages):
        if avg is None:
            continue
        has_values = True
        if direction == "up":
            if value > avg:
                count += 1
                opposite = 0
            elif value < avg:
                opposite += 1
                if opposite >= 2:
                    count = 0
            else:
                opposite = 0
        else:
            if value < avg:
                count += 1
                opposite = 0
            elif value > avg:
                opposite += 1
                if opposite >= 2:
                    count = 0
            else:
                opposite = 0
    return None if not has_values else count


def _pct_change(current: float | None, previous: float | None) -> float | None:
    if current is None or previous is None or previous == 0:
        return None
    return current / previous - 1


def _safe_float(value) -> float | None:
    if value is None:
        return None
    try:
        result = float(value)
    except (TypeError, ValueError):
        return None
    if pd.isna(result):
        return None
    return result


def _compute_volume_ratio(volumes: list[float], period: int = 20) -> float | None:
    if len(volumes) < period:
        return None
    window = volumes[-period:]
    avg = sum(window) / period
    if avg <= 0:
        return None
    return volumes[-1] / avg


def _detect_body_box(monthly_rows: list[tuple]) -> dict | None:
    min_months = 3
    max_months = 14
    max_range_pct = 0.2
    wild_wick_pct = 0.1

    bars: list[dict] = []
    for row in monthly_rows:
        if len(row) < 5:
            continue
        month_value, open_, high, low, close = row[:5]
        open_v = _safe_float(open_)
        high_v = _safe_float(high)
        low_v = _safe_float(low)
        close_v = _safe_float(close)
        if month_value is None or open_v is None or high_v is None or low_v is None or close_v is None:
            continue
        body_high = max(open_v, close_v)
        body_low = min(open_v, close_v)
        bars.append(
            {
                "time": int(month_value),
                "open": open_v,
                "high": high_v,
                "low": low_v,
                "close": close_v,
                "body_high": body_high,
                "body_low": body_low
            }
        )

    if len(bars) < min_months:
        return None

    bars.sort(key=lambda item: item["time"])
    max_months = min(max_months, len(bars))

    for length in range(max_months, min_months - 1, -1):
        window = bars[-length:]
        upper = max(item["body_high"] for item in window)
        lower = min(item["body_low"] for item in window)
        base = max(abs(lower), 1e-9)
        range_pct = (upper - lower) / base
        if range_pct > max_range_pct:
            continue
        wild = False
        for item in window:
            if item["high"] > upper * (1 + wild_wick_pct) or item["low"] < lower * (1 - wild_wick_pct):
                wild = True
                break
        return {
            "start": window[0]["time"],
            "end": window[-1]["time"],
            "upper": upper,
            "lower": lower,
            "months": length,
            "wild": wild,
            "range_pct": range_pct
        }

    return None


def compute_stage_score(
    daily_df: pd.DataFrame, monthly_df: pd.DataFrame
) -> tuple[str, float | None, str, list[str], dict]:
    missing_reasons: list[str] = []
    score_breakdown: dict[str, float] = {}

    daily = daily_df.sort_values("date")
    closes = [float(v) for v in daily["c"].tolist() if _safe_float(v) is not None]
    volumes = [float(v) if _safe_float(v) is not None else 0.0 for v in daily["v"].tolist()]

    last_close = closes[-1] if closes else None
    if last_close is None:
        missing_reasons.append("missing_last_close")

    if len(closes) < 60:
        missing_reasons.append("insufficient_daily_bars")

    ma7_series = _build_ma_series(closes, 7)
    ma20_series = _build_ma_series(closes, 20)
    ma60_series = _build_ma_series(closes, 60)
    ma100_series = _build_ma_series(closes, 100)

    ma7 = ma7_series[-1] if ma7_series else None
    ma20 = ma20_series[-1] if ma20_series else None
    ma60 = ma60_series[-1] if ma60_series else None
    ma100 = ma100_series[-1] if ma100_series else None

    if ma20 is None:
        missing_reasons.append("missing_ma20")
    if ma60 is None:
        missing_reasons.append("missing_ma60")
    if ma100 is None:
        missing_reasons.append("missing_ma100")

    slope20 = (
        ma20_series[-1] - ma20_series[-2]
        if len(ma20_series) >= 2 and ma20_series[-1] is not None and ma20_series[-2] is not None
        else None
    )
    slope60 = (
        ma60_series[-1] - ma60_series[-2]
        if len(ma60_series) >= 2 and ma60_series[-1] is not None and ma60_series[-2] is not None
        else None
    )

    monthly = monthly_df.sort_values("month")
    monthly_rows = monthly[["month", "o", "h", "l", "c"]].values.tolist()
    monthly_closes = [
        _safe_float(row[4]) for row in monthly_rows if len(row) >= 5 and _safe_float(row[4]) is not None
    ]

    if len(monthly_closes) < 3:
        missing_reasons.append("insufficient_monthly_bars")

    chg1m = _pct_change(monthly_closes[-1], monthly_closes[-2]) if len(monthly_closes) >= 2 else None
    chg1q = _pct_change(monthly_closes[-1], monthly_closes[-4]) if len(monthly_closes) >= 4 else None
    chg1y = _pct_change(monthly_closes[-1], monthly_closes[-13]) if len(monthly_closes) >= 13 else None

    if chg1m is None:
        missing_reasons.append("missing_chg1m")
    if chg1q is None:
        missing_reasons.append("missing_chg1q")
    if chg1y is None:
        missing_reasons.append("missing_chg1y")

    box = _detect_body_box(monthly_rows)
    if box is None and len(monthly_rows) >= 3:
        missing_reasons.append("no_box")

    box_active = False
    breakout_up = False
    if box and monthly_rows:
        latest_month = int(monthly_rows[-1][0])
        prev_month = int(monthly_rows[-2][0]) if len(monthly_rows) >= 2 else None
        if box["start"] <= latest_month <= box["end"]:
            box_active = True
        elif prev_month is not None and box["start"] <= prev_month <= box["end"]:
            box_active = True
        if box_active and last_close is not None and last_close > box["upper"]:
            breakout_up = True

    essential_missing = (
        last_close is None
        or ma20 is None
        or ma60 is None
        or len(closes) < 60
    )

    if essential_missing:
        return "UNKNOWN", None, "INSUFFICIENT_DATA", missing_reasons, score_breakdown

    up60 = _count_streak(closes, ma60_series, "up")
    down60 = _count_streak(closes, ma60_series, "down")
    down20 = _count_streak(closes, ma20_series, "down")
    up20 = _count_streak(closes, ma20_series, "up")

    stage = "B"
    if up60 is not None and (up60 >= 22 or (last_close > ma60 and (slope60 or 0) >= 0)):
        stage = "C"
    elif down60 is not None and down20 is not None and down60 >= 20 and down20 >= 10:
        stage = "A"

    trend = 0.0
    if last_close > ma20:
        trend += 8
    if ma20 > ma60:
        trend += 10
    if last_close > ma60:
        trend += 12
    if ma60 is not None and ma100 is not None and ma60 > ma100:
        trend += 10
    if slope20 is not None and slope20 > 0:
        trend += 3
    trend = min(40, trend)

    init_move = 0.0
    if len(closes) >= 2 and len(ma20_series) >= 2:
        prev_ma20 = ma20_series[-2]
        if prev_ma20 is not None and closes[-2] <= prev_ma20 and last_close > ma20:
            init_move += 15
    if breakout_up:
        init_move += 10
    init_move = min(25, init_move)

    base_build = 0.0
    if stage == "A" and ma20 is not None and last_close >= ma20 * 0.98:
        base_build += 8
    if ma7 is not None and len(ma7_series) >= 2 and ma7_series[-2] is not None:
        if ma7 > ma7_series[-2]:
            base_build += 4
    base_build = min(15, base_build)

    box_score = 0.0
    if breakout_up:
        box_score += 12
    elif box_active:
        box_score += 8
    box_score = min(15, box_score)

    volume_score = 0.0
    volume_ratio = _compute_volume_ratio(volumes, 20)
    if volume_ratio is not None and volume_ratio >= 1.5:
        volume_score = 5
    elif volume_ratio is not None and volume_ratio >= 1.1:
        volume_score = 2

    penalty = 0.0
    if up20 is not None and up20 >= 20:
        penalty -= 5
    if up60 is not None and up60 >= 22:
        penalty -= 10
    penalty = max(-20, penalty)

    score_breakdown = {
        "trend": trend,
        "init_move": init_move,
        "base_build": base_build,
        "box": box_score,
        "volume": volume_score,
        "penalty": penalty
    }

    score = trend + init_move + base_build + box_score + volume_score + penalty
    score = max(0.0, min(100.0, score))

    return stage, round(score, 3), "OK", missing_reasons, score_breakdown



def load_watchlist(data_dir: str) -> list[str]:
    path = find_code_txt_path(data_dir) if USE_CODE_TXT else None
    exists = bool(path and os.path.exists(path))
    if not USE_CODE_TXT:
        print("WATCHLIST_PATH=disabled exists=false count=0")
        return []
    if not path:
        print("WARNING: watchlist missing. WATCHLIST_PATH=none exists=false count=0")
        return []
    if not exists:
        print(f"WARNING: watchlist missing. WATCHLIST_PATH={path} exists=false count=0")
        return []
    try:
        with open(path, "r", encoding="utf-8") as f:
            codes = [line.strip() for line in f.readlines() if line.strip()]
        print(f"WATCHLIST_PATH={path} exists=true count={len(codes)}")
        return codes
    except OSError as exc:
        print(f"WARNING: watchlist read failed. WATCHLIST_PATH={path} exists=true count=0 reason={exc}")
        return []


def list_txt_files(data_dir: str) -> list[str]:
    if not os.path.isdir(data_dir):
        return []
    return [
        os.path.join(data_dir, name)
        for name in os.listdir(data_dir)
        if name.endswith(".txt") and name.lower() != "code.txt"
    ]


def read_csv_with_fallback(path: str) -> pd.DataFrame:
    encodings = ["utf-8", "shift_jis", "cp932"]
    last_err: Exception | None = None
    for encoding in encodings:
        try:
            return pd.read_csv(
                path,
                header=None,
                names=["code", "date", "o", "h", "l", "c", "v"],
                dtype="string",
                encoding=encoding,
                usecols=[0, 1, 2, 3, 4, 5, 6]
            )
        except Exception as exc:
            last_err = exc
    if last_err:
        raise last_err
    raise RuntimeError("Failed to read CSV")


def strip_header_row(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return df
    first = df.iloc[0].astype(str).str.lower().tolist()
    if len(first) >= 6 and first[0] in {"code", "ticker"} and "date" in first[1]:
        return df.iloc[1:].reset_index(drop=True)
    return df


def normalize_code(df: pd.DataFrame) -> tuple[pd.DataFrame, int, int, int]:
    df["code"] = df["code"].where(df["code"].notna(), "")
    df["code"] = df["code"].astype(str).str.strip()
    missing_mask = (df["code"] == "") | (df["code"].str.lower() == "nan")
    missing_count = int(missing_mask.sum())
    df = df[~missing_mask]

    nonstandard_mask = ~df["code"].str.match(CODE_PATTERN, na=False)
    nonstandard_count = int(nonstandard_mask.sum())
    invalid_count = 0
    if STRICT_CODE_VALIDATION and nonstandard_count:
        invalid_count = nonstandard_count
        df = df[~nonstandard_mask]

    return df, missing_count, nonstandard_count, invalid_count


def parse_file(path: str, watchlist: set[str] | None, counts: dict) -> pd.DataFrame:
    try:
        df = read_csv_with_fallback(path)
    except Exception as exc:
        counts["file_error"] += 1
        print(f"Warning: failed to read {path}: {exc}")
        return pd.DataFrame(columns=["code", "date", "o", "h", "l", "c", "v"])

    df = strip_header_row(df)
    if df.empty:
        return df

    df, missing_code, nonstandard_code, invalid_code = normalize_code(df)
    counts["missing_code"] += missing_code
    counts["nonstandard_code"] += nonstandard_code
    counts["invalid_code"] += invalid_code

    if df.empty:
        return df

    df["date"] = pd.to_datetime(df["date"], errors="coerce")
    invalid_date = int(df["date"].isna().sum())
    counts["invalid_date"] += invalid_date
    df = df[df["date"].notna()]
    if df.empty:
        return df

    if watchlist:
        before = len(df)
        df = df[df["code"].isin(watchlist)]
        counts["filtered_watchlist"] += int(before - len(df))

    if df.empty:
        return df

    for col in ["o", "h", "l", "c", "v"]:
        df[col] = pd.to_numeric(df[col], errors="coerce")

    non_numeric_mask = df[["o", "h", "l", "c", "v"]].isna().any(axis=1)
    counts["non_numeric"] += int(non_numeric_mask.sum())
    df = df[~non_numeric_mask]
    if df.empty:
        return df

    df["v"] = df["v"].round().astype("int64")
    return df


def read_daily_files(
    files: list[str], watchlist: set[str] | None, counts: dict
) -> tuple[pd.DataFrame, dict[str, str]]:
    latest_by_code: dict[str, tuple[float, pd.DataFrame]] = {}
    name_map: dict[str, str] = {}
    for path in files:
        df = parse_file(path, watchlist, counts)
        if df.empty:
            continue

        mtime = os.path.getmtime(path)
        for code, group in df.groupby("code"):
            existing = latest_by_code.get(code)
            if existing is None:
                latest_by_code[code] = (mtime, group)
                display_name = name_from_filename(path, code)
                if display_name:
                    name_map[code] = display_name
                continue
            if existing[0] >= mtime:
                counts["older_file"] += len(group)
                continue
            counts["older_file"] += len(existing[1])
            latest_by_code[code] = (mtime, group)
            display_name = name_from_filename(path, code)
            if display_name:
                name_map[code] = display_name

    frames = [entry[1] for entry in latest_by_code.values()]
    if not frames:
        return pd.DataFrame(columns=["code", "date", "o", "h", "l", "c", "v"]), name_map

    daily = pd.concat(frames, ignore_index=True)
    daily["date"] = daily["date"].dt.tz_localize("UTC")
    daily["date"] = (daily["date"].astype("int64") // 1_000_000_000).astype("int64")
    return daily, name_map


def build_monthly(daily: pd.DataFrame) -> pd.DataFrame:
    daily_dt = pd.to_datetime(daily["date"], unit="s", utc=True)
    daily = daily.assign(dt=daily_dt)
    daily["month"] = daily["dt"].dt.to_period("M").dt.to_timestamp()
    grouped = daily.sort_values("dt").groupby(["code", "month"], as_index=False)
    monthly = grouped.agg(
        o=("o", "first"),
        h=("h", "max"),
        l=("l", "min"),
        c=("c", "last")
    )
    monthly["month"] = (monthly["month"].astype("int64") // 1_000_000_000).astype("int64")
    return monthly


def build_monthly_ma(monthly: pd.DataFrame) -> pd.DataFrame:
    monthly = monthly.sort_values(["code", "month"]).copy()
    monthly["ma7"] = monthly.groupby("code")["c"].rolling(7).mean().reset_index(level=0, drop=True)
    monthly["ma20"] = monthly.groupby("code")["c"].rolling(20).mean().reset_index(level=0, drop=True)
    monthly["ma60"] = monthly.groupby("code")["c"].rolling(60).mean().reset_index(level=0, drop=True)
    return monthly[["code", "month", "ma7", "ma20", "ma60"]]


def build_daily_ma(daily: pd.DataFrame) -> pd.DataFrame:
    daily = daily.sort_values(["code", "date"]).copy()
    daily["ma7"] = daily.groupby("code")["c"].rolling(7).mean().reset_index(level=0, drop=True)
    daily["ma20"] = daily.groupby("code")["c"].rolling(20).mean().reset_index(level=0, drop=True)
    daily["ma60"] = daily.groupby("code")["c"].rolling(60).mean().reset_index(level=0, drop=True)
    return daily[["code", "date", "ma7", "ma20", "ma60"]]


def build_stock_meta(
    daily: pd.DataFrame,
    monthly: pd.DataFrame,
    name_map: dict[str, str]
) -> tuple[pd.DataFrame, dict]:
    now = datetime.now(tz=timezone.utc)
    records = []
    score_ok_count = 0
    score_insufficient_count = 0
    stage_counts: dict[str, int] = {}
    missing_reason_counts: dict[str, int] = {}

    daily_groups = {code: group for code, group in daily.groupby("code")}
    monthly_groups = {code: group for code, group in monthly.groupby("code")}

    for code, group in daily_groups.items():
        monthly_group = monthly_groups.get(code, pd.DataFrame(columns=monthly.columns))
        stage, score, score_status, missing_reasons, score_breakdown = compute_stage_score(
            group, monthly_group
        )
        stage_counts[stage] = stage_counts.get(stage, 0) + 1
        if score_status == "OK":
            score_ok_count += 1
        else:
            score_insufficient_count += 1
        for reason in missing_reasons:
            missing_reason_counts[reason] = missing_reason_counts.get(reason, 0) + 1
        records.append(
            {
                "code": code,
                "name": name_map.get(code, code),
                "stage": stage,
                "score": score,
                "reason": score_status,
                "score_status": score_status,
                "missing_reasons_json": json.dumps(missing_reasons, ensure_ascii=False),
                "score_breakdown_json": json.dumps(score_breakdown, ensure_ascii=False),
                "updated_at": now
            }
        )
    summary = {
        "score_ok": score_ok_count,
        "score_insufficient": score_insufficient_count,
        "stage_counts": stage_counts,
        "missing_reason_counts": missing_reason_counts
    }
    return pd.DataFrame(records), summary


def clear_tables() -> None:
    with get_conn() as conn:
        conn.execute("DELETE FROM daily_bars")
        conn.execute("DELETE FROM daily_ma")
        conn.execute("DELETE FROM monthly_bars")
        conn.execute("DELETE FROM monthly_ma")
        conn.execute("DELETE FROM stock_meta")
        conn.execute("DELETE FROM tickers")


def log_counts(counts: dict, parsed_rows: int) -> None:
    skipped_total = sum(
        counts[key]
        for key in [
            "missing_code",
            "invalid_date",
            "non_numeric",
            "invalid_code",
            "older_file",
            "filtered_watchlist"
        ]
    )
    reason_text = (
        f"missing_code={counts['missing_code']}, "
        f"invalid_date={counts['invalid_date']}, "
        f"non_numeric={counts['non_numeric']}, "
        f"invalid_code={counts['invalid_code']}, "
        f"older_file={counts['older_file']}, "
        f"filtered_watchlist={counts['filtered_watchlist']}"
    )
    print(f"PARSED_ROWS={parsed_rows}")
    print(f"SKIPPED_ROWS={skipped_total} ({reason_text})")
    print(f"NONSTANDARD_CODE_ROWS={counts['nonstandard_code']}")
    print(f"FILE_ERRORS={counts['file_error']}")


def ingest() -> None:
    def step_start(label: str) -> float:
        print(f"[STEP_START] {label}")
        return time.perf_counter()

    def step_end(label: str, start: float, **stats) -> None:
        elapsed_ms = int((time.perf_counter() - start) * 1000)
        stats_text = " ".join(
            f"{key}={value}" for key, value in stats.items() if value is not None
        )
        if stats_text:
            print(f"[STEP_END] {label} ms={elapsed_ms} {stats_text}")
        else:
            print(f"[STEP_END] {label} ms={elapsed_ms}")

    total_start = time.perf_counter()

    start = step_start("init_schema")
    init_schema()
    step_end("init_schema", start)

    start = step_start("list_txt_files")
    print(f"TXT_DIR={DATA_DIR}")
    files = list_txt_files(DATA_DIR)
    total_bytes = 0
    for path in files:
        try:
            total_bytes += os.path.getsize(path)
        except OSError:
            pass
    step_end("list_txt_files", start, file_count=len(files), total_bytes=total_bytes)

    counts = {
        "missing_code": 0,
        "invalid_date": 0,
        "non_numeric": 0,
        "invalid_code": 0,
        "older_file": 0,
        "filtered_watchlist": 0,
        "nonstandard_code": 0,
        "file_error": 0
    }

    if not files:
        clear_tables()
        log_counts(counts, 0)
        print("No TXT data found. Tables cleared.")
        total_ms = int((time.perf_counter() - total_start) * 1000)
        print(f"[STEP_END] ingest_total ms={total_ms} rows=0")
        return

    start = step_start("load_watchlist")
    watchlist = load_watchlist(DATA_DIR)
    step_end("load_watchlist", start, watchlist_count=len(watchlist))

    start = step_start("read_daily_files")
    daily, name_map = read_daily_files(files, watchlist, counts)
    daily_rows = len(daily)
    daily_codes = int(daily["code"].nunique()) if not daily.empty else 0
    step_end("read_daily_files", start, daily_rows=daily_rows, daily_codes=daily_codes)

    if daily.empty:
        clear_tables()
        log_counts(counts, 0)
        print("No valid TXT rows found. Tables cleared.")
        total_ms = int((time.perf_counter() - total_start) * 1000)
        print(f"[STEP_END] ingest_total ms={total_ms} rows=0")
        return

    start = step_start("build_monthly")
    monthly = build_monthly(daily)
    step_end("build_monthly", start, monthly_rows=len(monthly))

    start = step_start("build_monthly_ma")
    monthly_ma = build_monthly_ma(monthly)
    step_end("build_monthly_ma", start, monthly_ma_rows=len(monthly_ma))

    start = step_start("build_daily_ma")
    daily_ma = build_daily_ma(daily)
    step_end("build_daily_ma", start, daily_ma_rows=len(daily_ma))

    start = step_start("build_stock_meta")
    meta, meta_summary = build_stock_meta(daily, monthly, name_map)
    step_end("build_stock_meta", start, meta_rows=len(meta), score_ok=meta_summary.get("score_ok"), score_insufficient=meta_summary.get("score_insufficient"))
    if meta_summary.get("stage_counts"):
        stage_counts = ",".join(
            f"{key}:{value}" for key, value in sorted(meta_summary["stage_counts"].items())
        )
        print(f"STAGE_COUNTS={stage_counts}")
    if meta_summary.get("missing_reason_counts"):
        missing_sorted = sorted(
            meta_summary["missing_reason_counts"].items(),
            key=lambda item: item[1],
            reverse=True
        )[:10]
        missing_text = ",".join(f"{key}:{value}" for key, value in missing_sorted)
        print(f"MISSING_REASONS_TOP={missing_text}")

    start = step_start("db_replace")
    with get_conn() as conn:
        conn.execute("DELETE FROM daily_bars")
        conn.execute("DELETE FROM daily_ma")
        conn.execute("DELETE FROM monthly_bars")
        conn.execute("DELETE FROM monthly_ma")
        conn.execute("DELETE FROM stock_meta")
        conn.execute("DELETE FROM tickers")

        conn.register("daily_df", daily)
        conn.execute("INSERT INTO daily_bars SELECT code, date, o, h, l, c, v FROM daily_df")

        conn.register("daily_ma_df", daily_ma)
        conn.execute("INSERT INTO daily_ma SELECT code, date, ma7, ma20, ma60 FROM daily_ma_df")

        conn.register("monthly_df", monthly)
        conn.execute("INSERT INTO monthly_bars SELECT code, month, o, h, l, c FROM monthly_df")

        conn.register("monthly_ma_df", monthly_ma)
        conn.execute("INSERT INTO monthly_ma SELECT code, month, ma7, ma20, ma60 FROM monthly_ma_df")

        conn.register("meta_df", meta)
        conn.execute(
            "INSERT INTO stock_meta SELECT code, name, stage, score, reason, score_status, missing_reasons_json, score_breakdown_json, updated_at FROM meta_df"
        )

        conn.execute("INSERT INTO tickers SELECT code, name FROM meta_df")
    step_end("db_replace", start, daily_rows=len(daily), monthly_rows=len(monthly), meta_rows=len(meta))

    log_counts(counts, len(daily))
    print(f"Inserted {len(meta)} tickers")
    print(f"Inserted {len(monthly)} monthly rows")
    print(f"Inserted {len(daily)} daily rows")
    total_ms = int((time.perf_counter() - total_start) * 1000)
    print(f"[STEP_END] ingest_total ms={total_ms} rows={len(daily)}")


def main() -> None:

    ingest()


if __name__ == "__main__":
    main()
