from datetime import datetime, timedelta
import calendar
import csv
import glob
import json
import os
import re
import shutil
import sqlite3
import subprocess
import sys
import threading
import time
import traceback
import uuid

from fastapi import Body, FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse

from db import get_conn, init_schema
from box_detector import detect_boxes

REPO_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
DEFAULT_PAN_VBS_PATH = os.path.join(REPO_ROOT, "tools", "export_pan.vbs")
DEFAULT_PAN_CODE_PATH = os.path.join(REPO_ROOT, "tools", "code.txt")
DEFAULT_PAN_OUT_DIR = os.path.join(REPO_ROOT, "data", "txt")
APP_VERSION = os.getenv("APP_VERSION", "dev")
APP_ENV = os.getenv("APP_ENV") or os.getenv("ENV") or "dev"
DEBUG = os.getenv("DEBUG", "0") == "1"


def resolve_data_dir() -> str:
    env = os.getenv("PAN_OUT_TXT_DIR") or os.getenv("TXT_DATA_DIR")
    if env:
        return os.path.abspath(env)
    return os.path.abspath(DEFAULT_PAN_OUT_DIR)


DATA_DIR = resolve_data_dir()
DEFAULT_TRADE_RAKUTEN_PATH = os.path.abspath(
    os.path.join(os.path.dirname(__file__), "..", "..", "data", "楽天証券取引履歴.csv")
)
DEFAULT_TRADE_SBI_PATH = os.path.abspath(
    os.path.join(os.path.dirname(__file__), "..", "..", "data", "SBI証券取引履歴.csv")
)


def resolve_trade_csv_paths() -> list[str]:
    env = os.getenv("TRADE_CSV_PATH")
    if env:
        parts = [p.strip() for p in re.split(r"[;,\\n]+", env) if p.strip()]
        return [os.path.abspath(part) for part in parts]
    paths: list[str] = []
    if os.path.isfile(DEFAULT_TRADE_RAKUTEN_PATH):
        paths.append(DEFAULT_TRADE_RAKUTEN_PATH)
    if os.path.isfile(DEFAULT_TRADE_SBI_PATH):
        paths.append(DEFAULT_TRADE_SBI_PATH)
    if not paths:
        paths.append(DEFAULT_TRADE_RAKUTEN_PATH)
    return paths


TRADE_CSV_PATHS = resolve_trade_csv_paths()
USE_CODE_TXT = os.getenv("USE_CODE_TXT", "0") == "1"
DEFAULT_DB_PATH = os.getenv("STOCKS_DB_PATH", os.path.join(os.path.dirname(__file__), "stocks.duckdb"))
RANK_CONFIG_PATH = os.getenv("RANK_CONFIG_PATH", os.path.join(os.path.dirname(__file__), "rank_config.json"))
FAVORITES_DB_PATH = os.getenv(
    "FAVORITES_DB_PATH", os.path.join(os.path.dirname(__file__), "favorites.sqlite")
)
PRACTICE_DB_PATH = os.getenv(
    "PRACTICE_DB_PATH", os.path.join(os.path.dirname(__file__), "practice.sqlite")
)
PAN_EXPORT_VBS_PATH = os.path.abspath(
    os.getenv("PAN_EXPORT_VBS_PATH") or os.getenv("UPDATE_VBS_PATH") or DEFAULT_PAN_VBS_PATH
)
PAN_CODE_TXT_PATH = os.path.abspath(
    os.getenv("PAN_CODE_TXT_PATH") or DEFAULT_PAN_CODE_PATH
)
PAN_OUT_TXT_DIR = os.path.abspath(
    os.getenv("PAN_OUT_TXT_DIR") or DEFAULT_PAN_OUT_DIR
)
UPDATE_VBS_PATH = PAN_EXPORT_VBS_PATH
INGEST_SCRIPT_PATH = os.getenv(
    "INGEST_SCRIPT_PATH",
    os.path.abspath(os.path.join(os.path.dirname(__file__), "ingest_txt.py"))
)
UPDATE_STATE_PATH = os.getenv(
    "UPDATE_STATE_PATH",
    os.path.abspath(os.path.join(os.path.dirname(__file__), "update_state.json"))
)
SPLIT_SUSPECTS_PATH = os.path.abspath(os.path.join(DATA_DIR, "_split_suspects.csv"))
WATCHLIST_TRASH_DIR = os.path.abspath(os.path.join(DATA_DIR, "trash"))
WATCHLIST_TRASH_PATTERNS = [
    pattern
    for pattern in re.split(
        r"[;\n]+",
        os.getenv(
            "WATCHLIST_TRASH_PATTERNS",
            os.path.join(REPO_ROOT, "data", "csv", "{code}*.csv")
            + ";"
            + os.path.join(REPO_ROOT, "data", "txt", "{code}*.txt")
        )
    )
    if pattern.strip()
]
WATCHLIST_CODE_RE = re.compile(r"^\d{4}[A-Z]?$")
_watchlist_lock = threading.Lock()


def _build_name_map_from_txt() -> dict[str, str]:
    if not os.path.isdir(PAN_OUT_TXT_DIR):
        return {}
    name_map: dict[str, str] = {}
    for filename in os.listdir(PAN_OUT_TXT_DIR):
        if not filename.endswith(".txt") or filename.lower() == "code.txt":
            continue
        base = os.path.splitext(filename)[0]
        if "_" not in base:
            continue
        code, name = base.split("_", 1)
        code = code.strip()
        name = name.strip()
        if code and name and code not in name_map:
            name_map[code] = name
    return name_map


_trade_cache = {"key": None, "rows": [], "warnings": []}
_screener_cache = {"mtime": None, "rows": []}
_rank_cache = {"mtime": None, "config_mtime": None, "weekly": {}, "monthly": {}}
_rank_config_cache = {"mtime": None, "config": None}
_update_txt_lock = threading.Lock()
_update_txt_status = {
    "running": False,
    "phase": "idle",
    "started_at": None,
    "finished_at": None,
    "processed": 0,
    "total": 0,
    "summary": {},
    "error": None,
    "stdout_tail": [],
    "last_updated_at": None
}


def _get_favorites_conn():
    conn = sqlite3.connect(FAVORITES_DB_PATH)
    conn.row_factory = sqlite3.Row
    return conn


def _init_favorites_schema() -> None:
    with _get_favorites_conn() as conn:
        conn.execute(
            """
            CREATE TABLE IF NOT EXISTS favorites (
                code TEXT PRIMARY KEY,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
            """
        )


def _get_practice_conn():
    conn = sqlite3.connect(PRACTICE_DB_PATH)
    conn.row_factory = sqlite3.Row
    return conn


def _init_practice_schema() -> None:
    with _get_practice_conn() as conn:
        conn.execute(
            """
            CREATE TABLE IF NOT EXISTS practice_sessions (
                session_id TEXT PRIMARY KEY,
                code TEXT NOT NULL,
                start_date TEXT,
                end_date TEXT,
                cursor_time INTEGER,
                max_unlocked_time INTEGER,
                lot_size INTEGER,
                range_months INTEGER,
                trades TEXT,
                notes TEXT,
                ui_state TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
            """
        )
        _ensure_practice_column(conn, "practice_sessions", "end_date", "TEXT")
        _ensure_practice_column(conn, "practice_sessions", "cursor_time", "INTEGER")
        _ensure_practice_column(conn, "practice_sessions", "max_unlocked_time", "INTEGER")
        _ensure_practice_column(conn, "practice_sessions", "lot_size", "INTEGER")
        _ensure_practice_column(conn, "practice_sessions", "range_months", "INTEGER")
        _ensure_practice_column(conn, "practice_sessions", "ui_state", "TEXT")


def _ensure_practice_column(conn: sqlite3.Connection, table: str, column: str, column_type: str) -> None:
    existing = conn.execute(f"PRAGMA table_info({table})").fetchall()
    if any(row[1] == column for row in existing):
        return
    conn.execute(f"ALTER TABLE {table} ADD COLUMN {column} {column_type}")


def _normalize_code(value: str | None) -> str:
    if value is None:
        return ""
    text = str(value).strip()
    if not text:
        return ""
    match = re.search(r"\d{4}", text)
    if match:
        return match.group(0)
    return text.upper()


def _classify_exception(exc: Exception) -> tuple[int, str, str]:
    detail = str(exc)
    lower = detail.lower()
    db_missing = not os.path.isfile(DEFAULT_DB_PATH)
    if "io error" in lower or "failed to open" in lower or "cannot open" in lower:
        return 503, "DB_OPEN_FAILED", "Database open failed"
    if db_missing:
        return 503, "DATA_NOT_INITIALIZED", "Data not initialized"
    if (
        "no such table" in lower
        or "does not exist" in lower
        or "catalog error" in lower
        or "table with name" in lower
    ):
        return 503, "DATA_NOT_INITIALIZED", "Data not initialized"
    if isinstance(exc, sqlite3.Error):
        return 500, "SQLITE_ERROR", "Database error"
    return 500, "UNHANDLED_EXCEPTION", "Internal server error"


def _build_error_payload(exc: Exception, trace_id: str) -> dict:
    status_code, error_code, message = _classify_exception(exc)
    payload = {
        "trace_id": trace_id,
        "error_code": error_code,
        "message": message,
        "detail": str(exc)
    }
    if DEBUG:
        payload["stack"] = traceback.format_exc()
    return payload


def _load_favorite_codes() -> list[str]:
    with _get_favorites_conn() as conn:
        rows = conn.execute("SELECT code FROM favorites ORDER BY code").fetchall()
    return [row["code"] for row in rows]


def _load_favorite_items() -> list[dict]:
    codes = _load_favorite_codes()
    if not codes:
        return []
    names_by_code: dict[str, str] = {}
    placeholders = ",".join(["?"] * len(codes))
    with get_conn() as conn:
        rows = conn.execute(
            f"SELECT code, name FROM tickers WHERE code IN ({placeholders})",
            codes
        ).fetchall()
    for row in rows:
        code = str(row[0])
        name = row[1] or code
        names_by_code[code] = name
    return [{"code": code, "name": names_by_code.get(code, code)} for code in codes]


def find_code_txt_path(data_dir: str) -> str | None:
    if os.path.exists(PAN_CODE_TXT_PATH):
        return PAN_CODE_TXT_PATH
    return None


def _normalize_watch_code(raw: str | None) -> str | None:
    if raw is None:
        return None
    text = str(raw).strip()
    if not text:
        return None
    fullwidth = str.maketrans(
        "０１２３４５６７８９ＡＢＣＤＥＦＧＨＩＪＫＬＭＮＯＰＱＲＳＴＵＶＷＸＹＺａｂｃｄｅｆｇｈｉｊｋｌｍｎｏｐｑｒｓｔｕｖｗｘｙｚ",
        "0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz"
    )
    text = text.translate(fullwidth)
    text = re.sub(r"\s+", "", text)
    text = text.upper()
    if not WATCHLIST_CODE_RE.match(text):
        return None
    return text


def _extract_code_from_line(line: str) -> str | None:
    stripped = line.strip()
    if not stripped:
        return None
    if stripped.startswith("#") or stripped.startswith("'"):
        return None
    token = re.split(r"[,\t ]+", stripped, maxsplit=1)[0]
    return _normalize_watch_code(token)


def _read_watchlist_lines(path: str) -> list[str]:
    for encoding in ("utf-8", "cp932"):
        try:
            with open(path, "r", encoding=encoding) as handle:
                return handle.read().splitlines()
        except OSError:
            continue
    return []


def _write_watchlist_lines(path: str, lines: list[str]) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    tmp_path = f"{path}.tmp"
    with open(tmp_path, "w", encoding="utf-8") as handle:
        handle.write("\n".join(lines))
        handle.write("\n")
    os.replace(tmp_path, path)


def _load_watchlist_codes(path: str) -> list[str]:
    lines = _read_watchlist_lines(path)
    seen: set[str] = set()
    codes: list[str] = []
    for line in lines:
        code = _extract_code_from_line(line)
        if not code or code in seen:
            continue
        seen.add(code)
        codes.append(code)
    return codes


def _update_watchlist_file(path: str, code: str, remove: bool) -> bool:
    lines = _read_watchlist_lines(path)
    seen: set[str] = set()
    updated: list[str] = []
    removed = False
    for line in lines:
        parsed = _extract_code_from_line(line)
        if not parsed:
            updated.append(line)
            continue
        if parsed == code and remove:
            removed = True
            continue
        if parsed in seen:
            continue
        seen.add(parsed)
        updated.append(parsed)

    if not remove and code not in seen:
        updated.append(code)
    _write_watchlist_lines(path, updated)
    return removed


def _trash_watchlist_artifacts(code: str) -> tuple[str | None, list[str]]:
    if not WATCHLIST_TRASH_PATTERNS:
        return None, []
    trashed: list[str] = []
    token = datetime.now().strftime("%Y%m%d_%H%M%S")
    trash_dir = os.path.join(WATCHLIST_TRASH_DIR, token)
    os.makedirs(trash_dir, exist_ok=True)
    manifest: list[dict] = []
    for pattern in WATCHLIST_TRASH_PATTERNS:
        expanded = pattern.format(code=code)
        for path in glob.glob(expanded):
            if not os.path.isfile(path):
                continue
            dest = os.path.join(trash_dir, os.path.basename(path))
            shutil.move(path, dest)
            trashed.append(path)
            manifest.append({"from": path, "to": dest})
    if manifest:
        manifest_path = os.path.join(trash_dir, "_manifest.json")
        with open(manifest_path, "w", encoding="utf-8") as handle:
            json.dump(manifest, handle, ensure_ascii=False, indent=2)
        return token, trashed
    return None, []


def _restore_watchlist_artifacts(token: str) -> list[str]:
    if not token:
        return []
    trash_dir = os.path.join(WATCHLIST_TRASH_DIR, token)
    manifest_path = os.path.join(trash_dir, "_manifest.json")
    if not os.path.isfile(manifest_path):
        return []
    try:
        with open(manifest_path, "r", encoding="utf-8") as handle:
            manifest = json.load(handle)
    except (OSError, json.JSONDecodeError):
        return []
    restored: list[str] = []
    for entry in manifest:
        src = entry.get("to")
        dest = entry.get("from")
        if not src or not dest:
            continue
        if not os.path.isfile(src):
            continue
        os.makedirs(os.path.dirname(dest), exist_ok=True)
        shutil.move(src, dest)
        restored.append(dest)
    return restored

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"]
)


@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    trace_id = str(uuid.uuid4())
    payload = {
        "trace_id": trace_id,
        "error_code": "HTTP_ERROR",
        "message": "Request failed",
        "detail": str(exc.detail)
    }
    if DEBUG:
        payload["stack"] = traceback.format_exc()
    return JSONResponse(status_code=exc.status_code, content=payload, headers={"X-Request-Id": trace_id})


@app.exception_handler(Exception)
async def unhandled_exception_handler(request: Request, exc: Exception):
    trace_id = str(uuid.uuid4())
    status_code, _, _ = _classify_exception(exc)
    payload = _build_error_payload(exc, trace_id)
    return JSONResponse(status_code=status_code, content=payload, headers={"X-Request-Id": trace_id})


@app.on_event("startup")
def on_startup():
    init_schema()
    _init_favorites_schema()
    _init_practice_schema()


def _parse_trade_csv() -> dict:
    warnings: list[dict] = []
    paths = resolve_trade_csv_paths()
    existing_paths = [path for path in paths if os.path.isfile(path)]
    if not existing_paths:
        missing = ", ".join(paths)
        warnings.append({"type": "trade_csv_missing", "message": f"trade_csv_missing:{missing}"})
        return {"rows": [], "warnings": warnings}

    key = tuple((path, os.path.getmtime(path)) for path in existing_paths)
    if _trade_cache["key"] == key:
        return {"rows": _trade_cache["rows"], "warnings": _trade_cache["warnings"]}

    rows: list[dict] = []

    def normalize_text(value: str | None) -> str:
        if value is None:
            return ""
        text = str(value).replace("\ufeff", "")
        if text.strip().lower() in ("nan", "none", "--"):
            return ""
        text = text.replace("\u3000", " ")
        return text.strip()

    def normalize_label(value: str | None) -> str:
        text = normalize_text(value)
        if not text:
            return ""
        return re.sub(r"\s+", "", text)

    def read_csv_rows(path: str, encoding: str) -> list[list[str]]:
        with open(path, "r", encoding=encoding, newline="") as handle:
            reader = csv.reader(handle)
            return list(reader)

    def to_float(value: str) -> float:
        try:
            return float(value.replace(",", ""))
        except ValueError:
            return 0.0

    def to_optional_float(value: str) -> float | None:
        text = normalize_text(value)
        if not text:
            return None
        try:
            return float(text.replace(",", ""))
        except ValueError:
            return None

    def parse_date(value: str) -> str | None:
        raw = normalize_text(value)
        if not raw:
            return None
        for fmt in ("%Y/%m/%d", "%Y-%m-%d", "%Y%m%d"):
            try:
                return datetime.strptime(raw, fmt).strftime("%Y-%m-%d")
            except ValueError:
                continue
        return None

    def find_sbi_header_index(rows_all: list[list[str]]) -> int | None:
        start = min(6, max(0, len(rows_all)))
        for idx in range(start, min(len(rows_all), start + 6)):
            row = rows_all[idx]
            if not row or not any(cell.strip() for cell in row):
                continue
            if "約定日" in row or "銘柄コード" in row:
                return idx
        return None

    def looks_like_sbi(rows_all: list[list[str]]) -> bool:
        for row in rows_all[:10]:
            if any("CSV作成日" in cell for cell in row):
                return True
        header_index = find_sbi_header_index(rows_all)
        if header_index is None:
            return False
        header_row = [cell.strip() for cell in rows_all[header_index]]
        if any(
            name in header_row
            for name in ("受渡金額/決済損益", "決済損益", "受渡金額", "手数料/諸経費等")
        ):
            return True
        if "取引" in header_row:
            trade_idx = header_row.index("取引")
            for row in rows_all[header_index + 1 : header_index + 50]:
                if trade_idx < len(row) and any(
                    key in row[trade_idx] for key in ("信用新規買", "信用返済売", "信用新規売", "信用返済買")
                ):
                    return True
        return False

    def parse_sbi_rows(rows_all: list[list[str]], encoding_used: str) -> dict:
        header_index = find_sbi_header_index(rows_all)
        if header_index is None:
            warnings.append({"type": "sbi_header_missing", "message": "sbi_header_missing"})
            return {"rows": [], "warnings": warnings}

        header = [cell.strip() for cell in rows_all[header_index]]

        def find_col(*names: str) -> int | None:
            for name in names:
                if name in header:
                    return header.index(name)
            return None

        col_trade_date = find_col("約定日")
        col_settle_date = find_col("受渡日")
        col_code = find_col("銘柄コード")
        col_name = find_col("銘柄")
        col_market = find_col("市場")
        col_trade = find_col("取引")
        col_account = find_col("預り")
        col_qty = find_col("約定数量", "数量")
        col_price = find_col("約定単価", "単価")
        col_fee = find_col("手数料/諸経費等", "手数料等")
        col_tax = find_col("税額")
        col_amount = find_col("受渡金額/決済損益", "決済損益", "受渡金額")

        dedup_keys: set[str] = set()
        for row_index, line in enumerate(rows_all[header_index + 1 :], start=1):
            if not line or col_trade_date is None or col_code is None:
                continue
            if not any(cell.strip() for cell in line):
                continue
            date_value = parse_date(line[col_trade_date]) if col_trade_date < len(line) else None
            code_raw = normalize_text(line[col_code]) if col_code < len(line) else ""
            if not date_value or not code_raw:
                continue
            code = _normalize_code(code_raw)
            if not code:
                continue

            name = normalize_text(line[col_name]) if col_name is not None and col_name < len(line) else ""
            market = normalize_text(line[col_market]) if col_market is not None and col_market < len(line) else ""
            account = normalize_text(line[col_account]) if col_account is not None and col_account < len(line) else ""
            trade_raw = normalize_text(line[col_trade]) if col_trade is not None and col_trade < len(line) else ""
            qty_raw = normalize_text(line[col_qty]) if col_qty is not None and col_qty < len(line) else ""
            price_raw = normalize_text(line[col_price]) if col_price is not None and col_price < len(line) else ""
            fee_raw = normalize_text(line[col_fee]) if col_fee is not None and col_fee < len(line) else ""
            tax_raw = normalize_text(line[col_tax]) if col_tax is not None and col_tax < len(line) else ""
            amount_raw = normalize_text(line[col_amount]) if col_amount is not None and col_amount < len(line) else ""
            settle_date = (
                parse_date(line[col_settle_date]) if col_settle_date is not None and col_settle_date < len(line) else None
            )

            qty_shares = to_float(qty_raw)
            if qty_shares <= 0:
                continue
            if qty_shares % 100 != 0:
                warnings.append(
                    {
                        "type": "non_100_shares",
                        "message": f"non_100_shares:{code}:{date_value}:{qty_shares}",
                        "code": code
                    }
                )
            price = to_optional_float(price_raw)
            fee = to_optional_float(fee_raw)
            tax = to_optional_float(tax_raw)
            amount = to_optional_float(amount_raw)
            realized_net = None
            if amount is not None:
                realized_net = amount
                if fee is not None:
                    realized_net -= fee
                if tax is not None:
                    realized_net -= tax

            trade_label = normalize_label(trade_raw)
            txn_type = ""
            event_kind = None
            if "信用新規買" in trade_label:
                txn_type = "OPEN_LONG"
                event_kind = "BUY_OPEN"
            elif "信用返済売" in trade_label:
                txn_type = "CLOSE_LONG"
                event_kind = "SELL_CLOSE"
            elif "信用新規売" in trade_label:
                txn_type = "OPEN_SHORT"
                event_kind = "SELL_OPEN"
            elif "信用返済買" in trade_label:
                txn_type = "CLOSE_SHORT"
                event_kind = "BUY_CLOSE"
            elif "現物買" in trade_label or "買付" in trade_label:
                txn_type = "OPEN_LONG"
                event_kind = "BUY_OPEN"
            elif "現物売" in trade_label or "売付" in trade_label:
                txn_type = "CLOSE_LONG"
                event_kind = "SELL_CLOSE"
            elif "入庫" in trade_label:
                txn_type = "CORPORATE_ACTION"
                event_kind = "INBOUND"
            elif "出庫" in trade_label:
                txn_type = "CORPORATE_ACTION"
                event_kind = "OUTBOUND"

            if event_kind is None:
                sample = f"取引={trade_raw or '(blank)'}"
                unknown_labels_by_code.setdefault(code, set()).add(sample)
                continue

            dedup_key = "|".join([code, date_value, trade_label, qty_raw, price_raw, amount_raw])
            if dedup_key in dedup_keys:
                continue
            dedup_keys.add(dedup_key)

            if event_kind == "BUY_OPEN":
                side = "buy"
                action = "open"
            elif event_kind == "BUY_CLOSE":
                side = "buy"
                action = "close"
            elif event_kind == "SELL_OPEN":
                side = "sell"
                action = "open"
            elif event_kind == "SELL_CLOSE":
                side = "sell"
                action = "close"
            else:
                side = "buy"
                action = "open"

            if event_kind in ("BUY_OPEN", "SELL_OPEN"):
                event_order = 0
            elif event_kind in ("SELL_CLOSE", "BUY_CLOSE"):
                event_order = 1
            else:
                event_order = 2

            rows.append(
                {
                    "broker": "SBI",
                    "tradeDate": date_value,
                    "trade_date": date_value,
                    "settleDate": settle_date,
                    "settle_date": settle_date,
                    "code": code,
                    "name": name,
                    "market": market,
                    "account": account,
                    "txnType": txn_type,
                    "txn_type": txn_type,
                    "qty": qty_shares,
                    "qtyShares": qty_shares,
                    "units": int(qty_shares // 100),
                    "price": price if price is not None and price > 0 else None,
                    "fee": fee,
                    "tax": tax,
                    "realizedPnlGross": amount,
                    "realizedPnlNet": realized_net,
                    "memo": trade_raw,
                    "date": date_value,
                    "side": side,
                    "action": action,
                    "kind": event_kind,
                    "_row_index": row_index,
                    "_event_order": event_order,
                    "raw": {
                        "date": line[col_trade_date] if col_trade_date is not None and col_trade_date < len(line) else "",
                        "code": code_raw,
                        "name": name,
                        "trade": trade_raw,
                        "market": market,
                        "account": account,
                        "qty": qty_raw,
                        "price": price_raw,
                        "fee": fee_raw,
                        "tax": tax_raw,
                        "amount": amount_raw,
                        "encoding": encoding_used
                    }
                }
            )

        for code, samples_set in unknown_labels_by_code.items():
            samples = sorted(list(samples_set))[:5]
            warnings.append(
                {
                    "type": "unrecognized_labels",
                    "count": len(samples_set),
                    "samples": samples,
                    "code": code
                }
            )

        rows.sort(
            key=lambda item: (item.get("date", ""), item.get("_event_order", 2), item.get("_row_index", 0))
        )
        return {"rows": rows, "warnings": warnings}

    def parse_single(path: str) -> tuple[list[dict], list[dict]]:
        file_rows: list[dict] = []
        file_warnings: list[dict] = []
        unknown_labels_by_code: dict[str, set[str]] = {}

        try:
            rows_all = read_csv_rows(path, "cp932")
            encoding_used = "cp932"
        except UnicodeDecodeError:
            rows_all = read_csv_rows(path, "utf-8-sig")
            encoding_used = "utf-8-sig"

        if rows_all:
            header = [normalize_text(cell) for cell in rows_all[0]] if rows_all else []
            if not looks_like_sbi(rows_all) and ("約定日" not in header and "約定日付" not in header):
                try:
                    rows_all = read_csv_rows(path, "utf-8-sig")
                    encoding_used = "utf-8-sig"
                except UnicodeDecodeError:
                    pass

        if looks_like_sbi(rows_all):
            header_index = find_sbi_header_index(rows_all)
            if header_index is None:
                file_warnings.append(
                    {"type": "sbi_header_missing", "message": f"sbi_header_missing:{path}"}
                )
                return file_rows, file_warnings
            raw_header = [normalize_text(cell) for cell in rows_all[header_index]]
            data_rows = rows_all[header_index + 1 :]
            header = raw_header
            col_map = {name: index for index, name in enumerate(header) if name}
            get_cell = lambda row, key: normalize_text(row[col_map.get(key, -1)]) if key in col_map else ""

            for row_index, row in enumerate(data_rows, start=1):
                if not row or not any(cell.strip() for cell in row):
                    continue
                trade_date = parse_date(get_cell(row, "約定日"))
                if not trade_date:
                    continue
                code = _normalize_code(get_cell(row, "銘柄コード"))
                name = get_cell(row, "銘柄")
                market = get_cell(row, "市場")
                account = get_cell(row, "預り")
                trade_kind = get_cell(row, "取引") or get_cell(row, "取引区分")
                qty_raw = get_cell(row, "約定数量") or get_cell(row, "数量")
                qty_shares = to_float(qty_raw)
                price_raw = get_cell(row, "約定単価") or get_cell(row, "単価")
                price = to_optional_float(price_raw)
                fee_raw = get_cell(row, "手数料/諸経費等")
                tax_raw = get_cell(row, "税金") or get_cell(row, "税額")
                pnl_raw = get_cell(row, "受渡金額/決済損益") or get_cell(row, "決済損益")
                realized_pnl = to_optional_float(pnl_raw)
                if qty_shares <= 0:
                    continue

                event_kind = None
                if "信用新規買" in trade_kind:
                    event_kind = "BUY_OPEN"
                elif "信用返済売" in trade_kind:
                    event_kind = "SELL_CLOSE"
                elif "信用新規売" in trade_kind:
                    event_kind = "SELL_OPEN"
                elif "信用返済買" in trade_kind:
                    event_kind = "BUY_CLOSE"
                elif "現物買" in trade_kind or "買付" in trade_kind:
                    event_kind = "BUY_OPEN"
                elif "現物売" in trade_kind or "売付" in trade_kind:
                    event_kind = "SELL_CLOSE"

                if event_kind is None:
                    sample = f"取引区分={trade_kind or '(blank)'}, 売買区分=(blank)"
                    unknown_labels_by_code.setdefault(code, set()).add(sample)
                    continue

                if event_kind == "BUY_OPEN":
                    side = "buy"
                    action = "open"
                elif event_kind == "BUY_CLOSE":
                    side = "buy"
                    action = "close"
                elif event_kind == "SELL_OPEN":
                    side = "sell"
                    action = "open"
                else:
                    side = "sell"
                    action = "close"

                if event_kind in ("BUY_OPEN", "SELL_OPEN"):
                    event_order = 0
                elif event_kind in ("SELL_CLOSE", "BUY_CLOSE"):
                    event_order = 1
                else:
                    event_order = 2

                txn_type = "CORPORATE_ACTION"
                if event_kind == "BUY_OPEN":
                    txn_type = "OPEN_LONG"
                elif event_kind == "SELL_CLOSE":
                    txn_type = "CLOSE_LONG"
                elif event_kind == "SELL_OPEN":
                    txn_type = "OPEN_SHORT"
                elif event_kind == "BUY_CLOSE":
                    txn_type = "CLOSE_SHORT"

                file_rows.append(
                    {
                        "broker": "SBI",
                        "tradeDate": trade_date,
                        "trade_date": trade_date,
                        "settleDate": parse_date(get_cell(row, "受渡日")),
                        "settle_date": parse_date(get_cell(row, "受渡日")),
                        "date": trade_date,
                        "code": code,
                        "name": name,
                        "market": market,
                        "account": account,
                        "txnType": txn_type,
                        "txn_type": txn_type,
                        "qty": qty_shares,
                        "side": side,
                        "action": action,
                        "kind": event_kind,
                        "qtyShares": qty_shares,
                        "units": int(qty_shares // 100),
                        "price": price if price is not None and price > 0 else None,
                        "fee": to_optional_float(fee_raw),
                        "tax": to_optional_float(tax_raw),
                        "realizedPnlGross": realized_pnl,
                        "realizedPnlNet": realized_pnl,
                        "memo": trade_kind,
                        "_row_index": row_index,
                        "_event_order": event_order,
                        "raw": {
                            "date": trade_date,
                            "code": code,
                            "name": name,
                            "trade": trade_kind,
                            "qty": qty_raw,
                            "price": price_raw,
                            "amount": pnl_raw,
                            "encoding": encoding_used
                        }
                    }
                )

        else:
            rows_all = rows_all
            header = [normalize_text(cell) for cell in rows_all[0]] if rows_all else []
            data_rows = rows_all[1:] if rows_all else []
            col_map = {name: index for index, name in enumerate(header) if name}
            get_cell = lambda row, key: normalize_text(row[col_map.get(key, -1)]) if key in col_map else ""

            dedup_keys: set[str] = set()
            duplicate_counts: dict[str, int] = {}

            for row_index, row in enumerate(data_rows, start=1):
                if not row or not any(cell.strip() for cell in row):
                    continue
                date_raw = get_cell(row, "約定日") or get_cell(row, "日付")
                date_value = parse_date(date_raw)
                if not date_value:
                    continue
                settle_date = parse_date(get_cell(row, "受渡日"))
                code_raw = get_cell(row, "銘柄コード") or get_cell(row, "銘柄ｺｰﾄﾞ") or get_cell(row, "銘柄")
                code = _normalize_code(code_raw)
                name = get_cell(row, "銘柄名") or get_cell(row, "銘柄")
                market = get_cell(row, "市場")
                account = get_cell(row, "口座区分") or get_cell(row, "預り区分")
                type_raw = get_cell(row, "取引区分")
                kind_raw = get_cell(row, "売買区分")
                trade_type = normalize_label(type_raw)
                trade_kind = normalize_label(kind_raw)
                qty_raw = (
                    get_cell(row, "数量［株］")
                    or get_cell(row, "数量[株]")
                    or get_cell(row, "数量")
                    or get_cell(row, "数量(株)")
                )
                qty_shares = to_float(qty_raw)
                price_raw = (
                    get_cell(row, "単価［円］")
                    or get_cell(row, "単価[円]")
                    or get_cell(row, "単価")
                    or get_cell(row, "約定単価")
                )
                price = to_optional_float(price_raw)
                amount_raw = (
                    get_cell(row, "受渡金額［円］")
                    or get_cell(row, "受渡金額[円]")
                    or get_cell(row, "受渡金額")
                )
                fee_raw = (
                    get_cell(row, "手数料［円］")
                    or get_cell(row, "手数料[円]")
                    or get_cell(row, "手数料")
                )
                tax_raw = (
                    get_cell(row, "税金等［円］")
                    or get_cell(row, "税金等[円]")
                    or get_cell(row, "税金")
                )
                if qty_shares <= 0:
                    continue

                event_kind = None
                if trade_kind == "現渡" or trade_type == "現渡":
                    event_kind = "DELIVERY"
                elif trade_kind == "現引" or trade_type == "現引":
                    event_kind = "TAKE_DELIVERY"
                elif trade_type == "入庫" or trade_kind == "入庫":
                    event_kind = "INBOUND"
                elif trade_type == "出庫" or trade_kind == "出庫":
                    event_kind = "OUTBOUND"

                if event_kind is None:
                    if "買建" in trade_kind:
                        event_kind = "BUY_OPEN"
                    elif "売建" in trade_kind:
                        event_kind = "SELL_OPEN"
                    elif "買埋" in trade_kind:
                        event_kind = "BUY_CLOSE"
                    elif "売埋" in trade_kind:
                        event_kind = "SELL_CLOSE"
                    elif "現物買" in trade_kind or "買付" in trade_kind:
                        event_kind = "BUY_OPEN"
                    elif "現物売" in trade_kind or "売付" in trade_kind:
                        event_kind = "SELL_CLOSE"
                    elif trade_type == "入庫" or trade_kind == "入庫":
                        event_kind = "INBOUND"
                    elif trade_type == "出庫" or trade_kind == "出庫":
                        event_kind = "OUTBOUND"
                    elif trade_type == "現渡" or trade_kind == "現渡":
                        event_kind = "DELIVERY"
                    elif trade_type == "現引" or trade_kind == "現引":
                        event_kind = "TAKE_DELIVERY"

                if event_kind is None:
                    sample = f"取引区分={trade_type or '(blank)'}, 売買区分={trade_kind or '(blank)'}"
                    unknown_labels_by_code.setdefault(code, set()).add(sample)
                    continue

                dedup_key = "|".join(
                    [
                        code,
                        date_value,
                        trade_type,
                        trade_kind,
                        qty_raw,
                        price_raw,
                        amount_raw
                    ]
                )
                if dedup_key in dedup_keys:
                    duplicate_counts[code] = duplicate_counts.get(code, 0) + 1
                    continue
                dedup_keys.add(dedup_key)

                if event_kind == "BUY_OPEN":
                    side = "buy"
                    action = "open"
                elif event_kind == "BUY_CLOSE":
                    side = "buy"
                    action = "close"
                elif event_kind == "SELL_OPEN":
                    side = "sell"
                    action = "open"
                elif event_kind == "SELL_CLOSE":
                    side = "sell"
                    action = "close"
                else:
                    side = "buy"
                    action = "open"

                if event_kind in ("BUY_OPEN", "SELL_OPEN"):
                    event_order = 0
                elif event_kind in ("SELL_CLOSE", "BUY_CLOSE"):
                    event_order = 1
                else:
                    event_order = 2

                txn_type = "CORPORATE_ACTION"
                if event_kind == "BUY_OPEN":
                    txn_type = "OPEN_LONG"
                elif event_kind == "SELL_CLOSE":
                    txn_type = "CLOSE_LONG"
                elif event_kind == "SELL_OPEN":
                    txn_type = "OPEN_SHORT"
                elif event_kind == "BUY_CLOSE":
                    txn_type = "CLOSE_SHORT"

                file_rows.append(
                    {
                        "broker": "RAKUTEN",
                        "tradeDate": date_value,
                        "trade_date": date_value,
                        "settleDate": settle_date,
                        "settle_date": settle_date,
                        "date": date_value,
                        "code": code,
                        "name": name,
                        "market": market,
                        "account": account,
                        "txnType": txn_type,
                        "txn_type": txn_type,
                        "qty": qty_shares,
                        "side": side,
                        "action": action,
                        "kind": event_kind,
                        "qtyShares": qty_shares,
                        "units": int(qty_shares // 100),
                        "price": price if price is not None and price > 0 else None,
                        "fee": to_optional_float(fee_raw),
                        "tax": to_optional_float(tax_raw),
                        "realizedPnlGross": None,
                        "realizedPnlNet": None,
                        "memo": kind_raw or type_raw,
                        "_row_index": row_index,
                        "_event_order": event_order,
                        "raw": {
                            "date": date_raw,
                            "code": code_raw,
                            "name": name,
                            "trade": kind_raw,
                            "type": type_raw,
                            "qty": qty_raw,
                            "price": price_raw,
                            "amount": amount_raw,
                            "encoding": encoding_used
                        }
                    }
                )

            for code, count in duplicate_counts.items():
                file_warnings.append(
                    {"type": "duplicate_rows", "message": f"duplicate_rows:{code}:{count}", "code": code}
                )

        for code, samples_set in unknown_labels_by_code.items():
            samples = sorted(list(samples_set))[:5]
            file_warnings.append(
                {
                    "type": "unrecognized_labels",
                    "count": len(samples_set),
                    "samples": samples,
                    "code": code
                }
            )

        file_rows.sort(
            key=lambda item: (
                item.get("date", ""),
                item.get("_event_order", 2),
                item.get("_row_index", 0)
            )
        )
        return file_rows, file_warnings

    for path in existing_paths:
        file_rows, file_warnings = parse_single(path)
        if not file_rows and not file_warnings:
            continue
        rows.extend(file_rows)
        warnings.extend(file_warnings)

    rows.sort(
        key=lambda item: (item.get("date", ""), item.get("_event_order", 2), item.get("_row_index", 0))
    )

    _trade_cache["key"] = key
    _trade_cache["rows"] = rows
    _trade_cache["warnings"] = warnings
    return {"rows": rows, "warnings": warnings}


def _build_daily_positions(trades: list[dict]) -> list[dict]:
    grouped: dict[str, list[dict]] = {}
    for trade in trades:
        date = trade.get("date")
        if not date:
            continue
        grouped.setdefault(date, []).append(trade)

    long_shares = 0.0
    short_shares = 0.0
    positions: list[dict] = []

    def sort_key(item: dict) -> tuple[int, int]:
        return (item.get("_event_order", 2), item.get("_row_index", 0))

    for date in sorted(grouped.keys()):
        for trade in sorted(grouped[date], key=sort_key):
            qty_shares = float(trade.get("qtyShares") or 0)
            kind = trade.get("kind")
            if kind == "BUY_OPEN":
                long_shares += qty_shares
            elif kind == "SELL_CLOSE":
                long_shares = max(0.0, long_shares - qty_shares)
            elif kind == "SELL_OPEN":
                short_shares += qty_shares
            elif kind == "BUY_CLOSE":
                short_shares = max(0.0, short_shares - qty_shares)
            elif kind == "DELIVERY":
                long_shares = max(0.0, long_shares - qty_shares)
                short_shares = max(0.0, short_shares - qty_shares)
            elif kind == "TAKE_DELIVERY":
                continue
            elif kind == "INBOUND":
                if long_shares > 0 and qty_shares > 0:
                    long_shares += qty_shares
                continue
            elif kind == "OUTBOUND":
                if long_shares > 0 and qty_shares > 0:
                    long_shares = max(0.0, long_shares - qty_shares)
                continue

        positions.append(
            {
                "date": date,
                "buyShares": long_shares,
                "sellShares": short_shares,
                "buyUnits": long_shares / 100,
                "sellUnits": short_shares / 100,
                "text": f"{short_shares/100:g}-{long_shares/100:g}"
            }
        )

    return positions


def _strip_internal(row: dict) -> dict:
    return {key: value for key, value in row.items() if not key.startswith("_")}


def _build_warning_payload(warnings: list[dict], code: str | None = None) -> dict:
    items: list[str] = []
    unrecognized_count = 0
    unrecognized_samples: list[str] = []

    for warning in warnings:
        warning_code = warning.get("code")
        if code is not None and warning_code not in (None, code):
            continue
        if warning.get("type") == "unrecognized_labels":
            count = int(warning.get("count") or 0)
            samples = warning.get("samples") or []
            unrecognized_count += count
            for sample in samples:
                if sample in unrecognized_samples:
                    continue
                unrecognized_samples.append(sample)
                if len(unrecognized_samples) >= 5:
                    break
        else:
            message = warning.get("message") or warning.get("type") or ""
            if message:
                items.append(message)

    payload = {"items": items}
    if unrecognized_count:
        payload["unrecognized_labels"] = {
            "count": unrecognized_count,
            "samples": unrecognized_samples
        }
    return payload


def _parse_daily_date(value: int | str | None) -> datetime | None:
    if value is None:
        return None
    try:
        raw = str(int(value)).zfill(8)
        year = int(raw[:4])
        month = int(raw[4:6])
        day = int(raw[6:8])
        return datetime(year, month, day)
    except (ValueError, TypeError):
        return None


def _parse_practice_date(value: int | str | None) -> datetime | None:
    if value is None:
        return None
    if isinstance(value, (int, float)):
        try:
            numeric = int(value)
        except (TypeError, ValueError):
            return None
        if numeric >= 10_000_000_000_000:
            return datetime.utcfromtimestamp(numeric / 1000)
        if numeric >= 10_000_000_000:
            return datetime.utcfromtimestamp(numeric)
        if numeric >= 10_000_000:
            return _parse_daily_date(numeric)
        return None
    if isinstance(value, str):
        text = value.strip()
        if not text:
            return None
        if text.isdigit():
            return _parse_practice_date(int(text))
        match = re.match(r"^(\d{4})[/-](\d{1,2})[/-](\d{1,2})$", text)
        if match:
            try:
                year = int(match.group(1))
                month = int(match.group(2))
                day = int(match.group(3))
                return datetime(year, month, day)
            except ValueError:
                return None
    return None


def _format_practice_date(value: int | str | None) -> str | None:
    parsed = _parse_practice_date(value)
    if not parsed:
        return None
    return f"{parsed.year:04d}-{parsed.month:02d}-{parsed.day:02d}"


def _resolve_practice_start_date(session_id: str | None, start_date: int | str | None) -> datetime | None:
    if session_id:
        with _get_practice_conn() as conn:
            row = conn.execute(
                "SELECT start_date FROM practice_sessions WHERE session_id = ?",
                [session_id]
            ).fetchone()
        if row and row["start_date"]:
            parsed = _parse_practice_date(row["start_date"])
            if parsed:
                return parsed
    return _parse_practice_date(start_date)


def _parse_month_value(value: int | str | None) -> datetime | None:
    if value is None:
        return None
    try:
        raw = str(int(value)).zfill(6)
        year = int(raw[:4])
        month = int(raw[4:6])
        return datetime(year, month, 1)
    except (ValueError, TypeError):
        return None


def _format_month_label(value: int | str | None) -> str | None:
    month = _parse_month_value(value)
    if not month:
        return None
    return f"{month.year:04d}-{month.month:02d}"


def _month_label_to_int(label: str | None) -> int | None:
    if not label:
        return None
    try:
        parts = label.split("-")
        if len(parts) != 2:
            return None
        year = int(parts[0])
        month = int(parts[1])
        if month < 1 or month > 12:
            return None
        return year * 100 + month
    except (TypeError, ValueError):
        return None


def _pct_change(latest: float | None, prev: float | None) -> float | None:
    if latest is None or prev is None:
        return None
    if prev == 0:
        return None
    return (latest - prev) / prev * 100


def _build_weekly_bars(daily_rows: list[tuple]) -> list[dict]:
    items: list[dict] = []
    current_key = None
    for row in daily_rows:
        if len(row) < 5:
            continue
        date_value, open_, high, low, close = row[:5]
        if open_ is None or high is None or low is None or close is None:
            continue
        dt = _parse_daily_date(date_value)
        if not dt:
            continue
        week_start = (dt - timedelta(days=dt.weekday())).date()
        if current_key != week_start:
            items.append(
                {
                    "week_start": week_start,
                    "o": float(open_),
                    "h": float(high),
                    "l": float(low),
                    "c": float(close),
                    "last_date": dt.date()
                }
            )
            current_key = week_start
        else:
            current = items[-1]
            current["h"] = max(current["h"], float(high))
            current["l"] = min(current["l"], float(low))
            current["c"] = float(close)
            current["last_date"] = dt.date()
    return items


def _drop_incomplete_weekly(weekly: list[dict], last_daily: datetime | None) -> list[dict]:
    if not weekly or not last_daily:
        return weekly
    last_week_start = (last_daily - timedelta(days=last_daily.weekday())).date()
    if weekly[-1]["week_start"] == last_week_start and last_daily.weekday() < 4:
        return weekly[:-1]
    return weekly


def _drop_incomplete_monthly(monthly_rows: list[tuple], last_daily: datetime | None) -> list[tuple]:
    if not monthly_rows or not last_daily:
        return monthly_rows
    last_month = _parse_month_value(monthly_rows[-1][0] if monthly_rows else None)
    if last_month and last_month.year == last_daily.year and last_month.month == last_daily.month:
        return monthly_rows[:-1]
    return monthly_rows


def _build_quarterly_bars(monthly_rows: list[tuple]) -> list[dict]:
    items: list[dict] = []
    current_key: tuple[int, int] | None = None
    for row in monthly_rows:
        if len(row) < 5:
            continue
        month_value, open_, high, low, close = row[:5]
        dt = _parse_month_value(month_value)
        if not dt:
            continue
        quarter = (dt.month - 1) // 3 + 1
        key = (dt.year, quarter)
        if current_key != key:
            items.append(
                {
                    "year": dt.year,
                    "quarter": quarter,
                    "o": float(open_),
                    "h": float(high),
                    "l": float(low),
                    "c": float(close)
                }
            )
            current_key = key
        else:
            current = items[-1]
            current["h"] = max(current["h"], float(high))
            current["l"] = min(current["l"], float(low))
            current["c"] = float(close)
    return items


def _build_yearly_bars(monthly_rows: list[tuple]) -> list[dict]:
    items: list[dict] = []
    current_year = None
    for row in monthly_rows:
        if len(row) < 5:
            continue
        month_value, open_, high, low, close = row[:5]
        dt = _parse_month_value(month_value)
        if not dt:
            continue
        if current_year != dt.year:
            items.append(
                {
                    "year": dt.year,
                    "o": float(open_),
                    "h": float(high),
                    "l": float(low),
                    "c": float(close)
                }
            )
            current_year = dt.year
        else:
            current = items[-1]
            current["h"] = max(current["h"], float(high))
            current["l"] = min(current["l"], float(low))
            current["c"] = float(close)
    return items


def _build_ma_series(values: list[float], period: int) -> list[float | None]:
    if period <= 0:
        return [None for _ in values]
    result: list[float | None] = []
    total = 0.0
    for index, value in enumerate(values):
        total += value
        if index >= period:
            total -= values[index - period]
        if index >= period - 1:
            result.append(total / period)
        else:
            result.append(None)
    return result


def _count_streak(
    values: list[float],
    averages: list[float | None],
    direction: str
) -> int | None:
    count = 0
    opposite = 0
    has_values = False
    for value, avg in zip(values, averages):
        if avg is None:
            continue
        has_values = True
        if direction == "up":
            if value > avg:
                count += 1
                opposite = 0
            elif value < avg:
                opposite += 1
                if opposite >= 2:
                    count = 0
            else:
                opposite = 0
        else:
            if value < avg:
                count += 1
                opposite = 0
            elif value > avg:
                opposite += 1
                if opposite >= 2:
                    count = 0
            else:
                opposite = 0
    return None if not has_values else count


def _build_box_metrics(
    monthly_rows: list[tuple],
    last_close: float | None
) -> tuple[dict | None, str, str | None, str | None, str]:
    if not monthly_rows:
        return None, "NONE", None, None, "NONE"
    boxes = detect_boxes(monthly_rows)
    if not boxes:
        return None, "NONE", None, None, "NONE"

    bars = []
    for row in monthly_rows:
        if len(row) < 5:
            continue
        month_value, open_, high, low, close = row[:5]
        if open_ is None or close is None:
            continue
        bars.append(
            {
                "month": month_value,
                "open": float(open_),
                "close": float(close)
            }
        )

    if not bars:
        return None, "NONE", None, None, "NONE"

    latest_box = max(boxes, key=lambda item: item["endIndex"])
    months = latest_box["endIndex"] - latest_box["startIndex"] + 1
    if months < 3:
        return None, "NONE", None, None, "NONE"

    active_box = {**latest_box, "months": months}
    latest_index = len(bars) - 1
    start_index = active_box["startIndex"]
    end_index = active_box["endIndex"]
    body_low = None
    body_high = None
    for bar in bars[start_index : end_index + 1]:
        low = min(bar["open"], bar["close"])
        high = max(bar["open"], bar["close"])
        body_low = low if body_low is None else min(body_low, low)
        body_high = high if body_high is None else max(body_high, high)

    if body_low is None or body_high is None:
        return None, "NONE", None, None, "NONE"

    base = max(abs(body_low), 1e-9)
    range_pct = (body_high - body_low) / base
    start_label = _format_month_label(active_box["startTime"])
    end_label = _format_month_label(active_box["endTime"])

    box_state = "NONE"
    if end_index == latest_index:
        box_state = "IN_BOX"
    elif end_index == latest_index - 1:
        box_state = "JUST_BREAKOUT"

    breakout_month = None
    if box_state == "JUST_BREAKOUT" and latest_index >= 0:
        breakout_month = _format_month_label(bars[latest_index]["month"])

    direction_state = "NONE"
    if box_state != "NONE" and last_close is not None:
        if last_close > body_high:
            direction_state = "BREAKOUT_UP"
        elif last_close < body_low:
            direction_state = "BREAKOUT_DOWN"
        else:
            direction_state = "IN_BOX"

    payload = {
        "startDate": start_label,
        "endDate": end_label,
        "bodyLow": body_low,
        "bodyHigh": body_high,
        "months": active_box["months"],
        "rangePct": range_pct,
        "isActive": box_state == "IN_BOX",
        "boxState": box_state,
        "boxEndMonth": end_label,
        "breakoutMonth": breakout_month
    }
    return payload, box_state, end_label, breakout_month, direction_state


def _load_rank_config() -> dict:
    path = RANK_CONFIG_PATH
    mtime = os.path.getmtime(path) if os.path.isfile(path) else None
    cached = _rank_config_cache.get("config")
    if _rank_config_cache.get("mtime") == mtime and cached is not None:
        return cached
    config: dict = {}
    if mtime is not None:
        try:
            with open(path, "r", encoding="utf-8") as handle:
                config = json.load(handle) or {}
        except (OSError, json.JSONDecodeError):
            config = {}
    _rank_config_cache["mtime"] = mtime
    _rank_config_cache["config"] = config
    return config


def _get_config_value(config: dict, keys: list[str], default):
    current = config
    for key in keys:
        if not isinstance(current, dict) or key not in current:
            return default
        current = current[key]
    return current


def _parse_as_of_date(value: str | None) -> datetime | None:
    if value is None:
        return None
    text = str(value).strip()
    if not text:
        return None
    if re.match(r"^\d{8}$", text):
        try:
            year = int(text[:4])
            month = int(text[4:6])
            day = int(text[6:8])
            return datetime(year, month, day)
        except ValueError:
            return None
    if re.match(r"^\d{6}$", text):
        try:
            year = int(text[:4])
            month = int(text[4:6])
            return datetime(year, month, 1)
        except ValueError:
            return None
    for fmt in ("%Y-%m-%d", "%Y/%m/%d"):
        try:
            return datetime.strptime(text, fmt)
        except ValueError:
            continue
    return None


def _as_of_int(value: str | None) -> int | None:
    dt = _parse_as_of_date(value)
    if not dt:
        return None
    return dt.year * 10000 + dt.month * 100 + dt.day


def _as_of_month_int(value: str | None) -> int | None:
    dt = _parse_as_of_date(value)
    if not dt:
        return None
    return dt.year * 100 + dt.month


def _format_daily_label(value: int | None) -> str | None:
    if value is None:
        return None
    raw = str(int(value)).zfill(8)
    return f"{raw[:4]}-{raw[4:6]}-{raw[6:8]}"


def _parse_codes_from_text(text: str) -> list[str]:
    codes = re.findall(r"\d{4}", text)
    return sorted(set(codes))


def _load_universe_codes(universe: str | None) -> tuple[list[str], str | None, float | None]:
    if not universe:
        return [], None, None
    key = universe.strip().lower()
    if not key or key in ("all", "*"):
        return [], None, None

    path = None
    if key in ("watchlist", "code", "code.txt"):
        path = find_code_txt_path(DATA_DIR)
    else:
        candidates = [
            os.path.join(DATA_DIR, f"{universe}.txt"),
            os.path.join(os.path.dirname(DATA_DIR), f"{universe}.txt"),
            os.path.join(os.path.dirname(os.path.dirname(DATA_DIR)), f"{universe}.txt")
        ]
        for candidate in candidates:
            if os.path.isfile(candidate):
                path = candidate
                break

    if not path or not os.path.isfile(path):
        return [], None, None

    try:
        with open(path, "r", encoding="utf-8") as handle:
            text = handle.read()
    except OSError:
        return [], path, None

    codes = _parse_codes_from_text(text)
    mtime = os.path.getmtime(path) if os.path.isfile(path) else None
    return codes, path, mtime


def _resolve_universe_codes(conn, universe: str | None) -> tuple[list[str], dict]:
    all_codes = [row[0] for row in conn.execute(
        "SELECT DISTINCT code FROM daily_bars ORDER BY code"
    ).fetchall()]
    if not universe or universe.strip().lower() in ("", "all", "*"):
        return all_codes, {"source": "all", "requested": universe}

    universe_codes, path, mtime = _load_universe_codes(universe)
    if not universe_codes:
        return all_codes, {"source": "all", "requested": universe, "warning": "universe_not_found"}

    allowed = set(all_codes)
    filtered = [code for code in universe_codes if code in allowed]
    return filtered, {
        "source": "file",
        "requested": universe,
        "path": path,
        "mtime": mtime,
        "missing": len(universe_codes) - len(filtered)
    }


def _group_rows_by_code(rows: list[tuple]) -> dict[str, list[tuple]]:
    grouped: dict[str, list[tuple]] = {}
    for row in rows:
        if not row:
            continue
        code = row[0]
        grouped.setdefault(code, []).append(row[1:])
    return grouped


def _fetch_daily_rows(conn, codes: list[str], as_of: int | None, limit: int) -> dict[str, list[tuple]]:
    if not codes:
        return {}
    placeholders = ",".join(["?"] * len(codes))
    where_clauses = [f"code IN ({placeholders})"]
    params: list = list(codes)
    if as_of is not None:
        where_clauses.append("date <= ?")
        params.append(as_of)
    where_sql = " AND ".join(where_clauses)

    query = f"""
        SELECT code, date, o, h, l, c, v
        FROM (
            SELECT
                code,
                date,
                o,
                h,
                l,
                c,
                v,
                ROW_NUMBER() OVER (PARTITION BY code ORDER BY date DESC) AS rn
            FROM daily_bars
            WHERE {where_sql}
        )
        WHERE rn <= ?
        ORDER BY code, date
    """
    params.append(limit)
    rows = conn.execute(query, params).fetchall()
    return _group_rows_by_code(rows)


def _fetch_monthly_rows(conn, codes: list[str], as_of_month: int | None, limit: int) -> dict[str, list[tuple]]:
    if not codes:
        return {}
    placeholders = ",".join(["?"] * len(codes))
    where_clauses = [f"code IN ({placeholders})"]
    params: list = list(codes)
    if as_of_month is not None:
        where_clauses.append("month <= ?")
        params.append(as_of_month)
    where_sql = " AND ".join(where_clauses)
    query = f"""
        SELECT code, month, o, h, l, c
        FROM (
            SELECT
                code,
                month,
                o,
                h,
                l,
                c,
                ROW_NUMBER() OVER (PARTITION BY code ORDER BY month DESC) AS rn
            FROM monthly_bars
            WHERE {where_sql}
        )
        WHERE rn <= ?
        ORDER BY code, month
    """
    params.append(limit)
    rows = conn.execute(query, params).fetchall()
    return _group_rows_by_code(rows)


def _normalize_daily_rows(rows: list[tuple], as_of: int | None) -> list[tuple]:
    by_date: dict[int, tuple] = {}
    for row in rows:
        if len(row) < 6:
            continue
        date_value = row[0]
        if date_value is None:
            continue
        date_int = int(date_value)
        if as_of is not None and date_int > as_of:
            continue
        by_date[date_int] = row
    return [by_date[key] for key in sorted(by_date.keys())]


def _normalize_monthly_rows(rows: list[tuple], as_of_month: int | None) -> list[tuple]:
    by_month: dict[int, tuple] = {}
    for row in rows:
        if len(row) < 5:
            continue
        month_value = row[0]
        if month_value is None:
            continue
        month_int = int(month_value)
        if as_of_month is not None and month_int > as_of_month:
            continue
        by_month[month_int] = row
    return [by_month[key] for key in sorted(by_month.keys())]


def _compute_atr(highs: list[float], lows: list[float], closes: list[float], period: int) -> float | None:
    if len(closes) < 2 or len(closes) != len(highs) or len(closes) != len(lows):
        return None
    trs: list[float] = []
    prev_close = closes[0]
    for high, low, close in zip(highs, lows, closes):
        tr = max(high - low, abs(high - prev_close), abs(low - prev_close))
        trs.append(tr)
        prev_close = close
    if len(trs) < period:
        return None
    window = trs[-period:]
    return sum(window) / period


def _compute_volume_ratio(volumes: list[float], period: int, include_latest: bool) -> float | None:
    if period <= 0:
        return None
    if include_latest:
        if len(volumes) < period:
            return None
        window = volumes[-period:]
    else:
        if len(volumes) < period + 1:
            return None
        window = volumes[-period - 1:-1]
    avg = sum(window) / period if period else 0
    if avg <= 0:
        return None
    latest = volumes[-1]
    return latest / avg


def _calc_slope(values: list[float | None], lookback: int) -> float | None:
    if lookback <= 0 or len(values) <= lookback:
        return None
    current = values[-1]
    past = values[-1 - lookback]
    if current is None or past is None:
        return None
    return float(current) - float(past)


def _calc_recent_bounds(highs: list[float], lows: list[float], lookback: int) -> tuple[float | None, float | None]:
    if not highs or not lows:
        return None, None
    if lookback <= 0:
        return max(highs), min(lows)
    window_highs = highs[-lookback:] if len(highs) >= lookback else highs
    window_lows = lows[-lookback:] if len(lows) >= lookback else lows
    return max(window_highs), min(window_lows)


def _detect_body_box(monthly_rows: list[tuple], config: dict) -> dict | None:
    thresholds = _get_config_value(config, ["monthly", "thresholds"], {})
    min_months = int(thresholds.get("min_months", 3))
    max_months = int(thresholds.get("max_months", 14))
    max_range_pct = float(thresholds.get("max_range_pct", 0.2))
    wild_wick_pct = float(thresholds.get("wild_wick_pct", 0.1))

    bars: list[dict] = []
    for row in monthly_rows:
        if len(row) < 5:
            continue
        month_value, open_, high, low, close = row[:5]
        if month_value is None or open_ is None or high is None or low is None or close is None:
            continue
        body_high = max(float(open_), float(close))
        body_low = min(float(open_), float(close))
        bars.append(
            {
                "time": int(month_value),
                "open": float(open_),
                "high": float(high),
                "low": float(low),
                "close": float(close),
                "body_high": body_high,
                "body_low": body_low
            }
        )

    if len(bars) < min_months:
        return None

    bars.sort(key=lambda item: item["time"])
    max_months = min(max_months, len(bars))

    for length in range(max_months, min_months - 1, -1):
        window = bars[-length:]
        upper = max(item["body_high"] for item in window)
        lower = min(item["body_low"] for item in window)
        base = max(abs(lower), 1e-9)
        range_pct = (upper - lower) / base
        if range_pct > max_range_pct:
            continue
        wild = False
        for item in window:
            if item["high"] > upper * (1 + wild_wick_pct) or item["low"] < lower * (1 - wild_wick_pct):
                wild = True
                break
        return {
            "start": window[0]["time"],
            "end": window[-1]["time"],
            "upper": upper,
            "lower": lower,
            "months": length,
            "range_pct": range_pct,
            "wild": wild,
            "last_close": window[-1]["close"]
        }

    return None


def _score_weekly_candidate(code: str, name: str, rows: list[tuple], config: dict, as_of: int | None) -> tuple[dict | None, dict | None, str | None]:
    rows = _normalize_daily_rows(rows, as_of)
    common = _get_config_value(config, ["common"], {})
    min_bars = int(common.get("min_daily_bars", 80))
    if len(rows) < min_bars:
        return None, None, "insufficient_daily_bars"

    dates = [int(row[0]) for row in rows]
    opens = [float(row[1]) for row in rows]
    highs = [float(row[2]) for row in rows]
    lows = [float(row[3]) for row in rows]
    closes = [float(row[4]) for row in rows]
    volumes = [float(row[5]) if row[5] is not None else 0.0 for row in rows]

    close = closes[-1] if closes else None
    if close is None:
        return None, None, "missing_close"

    ma7_series = _build_ma_series(closes, 7)
    ma20_series = _build_ma_series(closes, 20)
    ma60_series = _build_ma_series(closes, 60)

    ma7 = ma7_series[-1] if ma7_series else None
    ma20 = ma20_series[-1] if ma20_series else None
    ma60 = ma60_series[-1] if ma60_series else None
    if ma20 is None or ma60 is None:
        return None, None, "missing_ma"

    slope_lookback = int(common.get("slope_lookback", 3))
    slope20 = _calc_slope(ma20_series, slope_lookback)

    atr_period = int(common.get("atr_period", 14))
    atr14 = _compute_atr(highs, lows, closes, atr_period)

    volume_period = int(common.get("volume_period", 20))
    include_latest = common.get("volume_ratio_mode", "exclude_latest") == "include_latest"
    volume_ratio = _compute_volume_ratio(volumes, volume_period, include_latest)

    up7 = _count_streak(closes, ma7_series, "up")
    down7 = _count_streak(closes, ma7_series, "down")

    trigger_lookback = int(common.get("trigger_lookback", 20))
    recent_high, recent_low = _calc_recent_bounds(highs, lows, trigger_lookback)
    break_up_pct = None
    break_down_pct = None
    if recent_high is not None and close:
        break_up_pct = max(0.0, (recent_high - close) / close * 100)
    if recent_low is not None and close:
        break_down_pct = max(0.0, (close - recent_low) / close * 100)

    weekly = _get_config_value(config, ["weekly"], {})
    weights = weekly.get("weights", {})
    thresholds = weekly.get("thresholds", {})
    down_weights = weekly.get("down_weights", {})
    down_thresholds = weekly.get("down_thresholds", {})
    max_reasons = int(common.get("max_reasons", 6))

    up_reasons: list[tuple[float, str]] = []
    down_reasons: list[tuple[float, str]] = []
    up_badges: list[str] = []
    down_badges: list[str] = []
    up_score = 0.0
    down_score = 0.0

    def push_reason(target: list[tuple[float, str]], weight: float, label: str):
        if weight:
            target.append((weight, label))

    def push_badge(target: list[str], label: str):
        if label and label not in target:
            target.append(label)

    if close > ma20 and ma20 > ma60:
        weight = float(weights.get("ma_alignment", 0))
        up_score += weight
        push_reason(up_reasons, weight, "MA20がMA60より上")
        push_badge(up_badges, "MA整列")

    pull_min = int(thresholds.get("pullback_down7_min", 1))
    pull_max = int(thresholds.get("pullback_down7_max", 2))
    slope_min = float(thresholds.get("slope_min", 0))
    if close > ma20 and down7 is not None and pull_min <= down7 <= pull_max:
        if slope20 is None or slope20 >= slope_min:
            weight = float(weights.get("pullback_above_ma20", 0))
            up_score += weight
            push_reason(up_reasons, weight, f"MA20上で押し目（下{down7}本）")
            push_badge(up_badges, "押し目")

    vol_thresh = float(thresholds.get("volume_ratio", 1.5))
    if volume_ratio is not None and volume_ratio >= vol_thresh:
        weight = float(weights.get("volume_spike", 0))
        up_score += weight
        push_reason(up_reasons, weight, f"出来高増（20日比{volume_ratio:.2f}倍）")
        push_badge(up_badges, "出来高増")

    near_pct = float(thresholds.get("near_break_pct", 2.0))
    if break_up_pct is not None and break_up_pct <= near_pct:
        weight = float(weights.get("near_high_break", 0))
        up_score += weight
        push_reason(up_reasons, weight, f"高値ブレイク接近（{break_up_pct:.1f}%）")
        push_badge(up_badges, "高値接近")

    if slope20 is not None and slope20 >= slope_min:
        weight = float(weights.get("slope_up", 0))
        up_score += weight
        push_reason(up_reasons, weight, "MA20上向き")
        push_badge(up_badges, "MA上向き")

    big_candle = float(thresholds.get("big_candle_atr", 1.2))
    if atr14 is not None and abs(close - opens[-1]) >= atr14 * big_candle and close > opens[-1]:
        weight = float(weights.get("big_bull_candle", 0))
        up_score += weight
        push_reason(up_reasons, weight, "強い陽線")
        push_badge(up_badges, "陽線強")

    ma20_dist = float(thresholds.get("ma20_distance_pct", 2.0))
    if ma20:
        dist_pct = abs(close - ma20) / ma20 * 100
        if close >= ma20 and dist_pct <= ma20_dist:
            weight = float(weights.get("ma20_support", 0))
            up_score += weight
            push_reason(up_reasons, weight, f"MA20近接（{dist_pct:.1f}%）")
            push_badge(up_badges, "MA20近接")

    if close < ma20 and ma20 < ma60:
        weight = float(down_weights.get("ma_alignment", 0))
        down_score += weight
        push_reason(down_reasons, weight, "MA20がMA60より下")
        push_badge(down_badges, "MA逆転")

    pull_min = int(down_thresholds.get("pullback_up7_min", 1))
    pull_max = int(down_thresholds.get("pullback_up7_max", 2))
    slope_max = float(down_thresholds.get("slope_max", 0))
    if close < ma20 and up7 is not None and pull_min <= up7 <= pull_max:
        if slope20 is None or slope20 <= slope_max:
            weight = float(down_weights.get("pullback_below_ma20", 0))
            down_score += weight
            push_reason(down_reasons, weight, f"MA20下で戻り（上{up7}本）")
            push_badge(down_badges, "戻り")

    vol_thresh = float(down_thresholds.get("volume_ratio", vol_thresh))
    if volume_ratio is not None and volume_ratio >= vol_thresh:
        weight = float(down_weights.get("volume_spike", 0))
        down_score += weight
        push_reason(down_reasons, weight, f"出来高増（20日比{volume_ratio:.2f}倍）")
        push_badge(down_badges, "出来高増")

    near_pct = float(down_thresholds.get("near_break_pct", near_pct))
    if break_down_pct is not None and break_down_pct <= near_pct:
        weight = float(down_weights.get("near_low_break", 0))
        down_score += weight
        push_reason(down_reasons, weight, f"安値ブレイク接近（{break_down_pct:.1f}%）")
        push_badge(down_badges, "安値接近")

    if slope20 is not None and slope20 <= slope_max:
        weight = float(down_weights.get("slope_down", 0))
        down_score += weight
        push_reason(down_reasons, weight, "MA20下向き")
        push_badge(down_badges, "MA下向き")

    big_candle = float(down_thresholds.get("big_candle_atr", big_candle))
    if atr14 is not None and abs(close - opens[-1]) >= atr14 * big_candle and close < opens[-1]:
        weight = float(down_weights.get("big_bear_candle", 0))
        down_score += weight
        push_reason(down_reasons, weight, "強い陰線")
        push_badge(down_badges, "陰線強")

    ma20_dist = float(down_thresholds.get("ma20_distance_pct", ma20_dist))
    if ma20:
        dist_pct = abs(close - ma20) / ma20 * 100
        if close <= ma20 and dist_pct <= ma20_dist:
            weight = float(down_weights.get("ma20_resistance", 0))
            down_score += weight
            push_reason(down_reasons, weight, f"MA20近接（{dist_pct:.1f}%）")
            push_badge(down_badges, "MA20近接")

    up_reasons.sort(key=lambda item: item[0], reverse=True)
    down_reasons.sort(key=lambda item: item[0], reverse=True)

    levels = {
        "close": close,
        "ma7": ma7,
        "ma20": ma20,
        "ma60": ma60,
        "atr14": atr14,
        "volume_ratio": volume_ratio
    }

    chart_hint = {
        "lines": {
            "ma20": ma20,
            "ma60": ma60,
            "recent_high": recent_high,
            "recent_low": recent_low
        }
    }

    as_of_label = _format_daily_label(dates[-1])
    series_bars = int(common.get("rank_series_bars", 60))
    series_rows = rows[-series_bars:] if series_bars > 0 else rows
    series = [
        [int(item[0]), float(item[1]), float(item[2]), float(item[3]), float(item[4])]
        for item in series_rows
    ]

    base = {
        "code": code,
        "name": name or code,
        "as_of": as_of_label,
        "levels": levels,
        "series": series,
        "distance_to_trigger": {
            "break_up_pct": break_up_pct,
            "break_down_pct": break_down_pct
        },
        "chart_hint": chart_hint
    }

    up_item = {
        **base,
        "total_score": round(up_score, 3),
        "reasons": [label for _, label in up_reasons[:max_reasons]],
        "badges": up_badges[:max_reasons]
    }
    down_item = {
        **base,
        "total_score": round(down_score, 3),
        "reasons": [label for _, label in down_reasons[:max_reasons]],
        "badges": down_badges[:max_reasons]
    }

    return up_item, down_item, None


def _score_monthly_candidate(code: str, name: str, rows: list[tuple], config: dict, as_of_month: int | None) -> tuple[dict | None, str | None]:
    rows = _normalize_monthly_rows(rows, as_of_month)
    thresholds = _get_config_value(config, ["monthly", "thresholds"], {})
    min_months = int(thresholds.get("min_months", 3))
    if len(rows) < min_months:
        return None, "insufficient_monthly_bars"

    box = _detect_body_box(rows, config)
    if not box:
        return None, "no_box"

    weights = _get_config_value(config, ["monthly", "weights"], {})
    max_reasons = int(_get_config_value(config, ["common", "max_reasons"], 6))
    near_edge_pct = float(thresholds.get("near_edge_pct", 4.0))
    wild_penalty = float(weights.get("wild_box_penalty", 0))

    close = float(box["last_close"])
    upper = float(box["upper"])
    lower = float(box["lower"])
    break_up_pct = max(0.0, (upper - close) / close * 100) if close else None
    break_down_pct = max(0.0, (close - lower) / close * 100) if close else None
    edge_pct = None
    if break_up_pct is not None and break_down_pct is not None:
        edge_pct = min(break_up_pct, break_down_pct)

    reasons: list[tuple[float, str]] = []
    score = 0.0

    months = int(box["months"])
    weight_month = float(weights.get("box_months", 0))
    if weight_month:
        score += weight_month * months
        reasons.append((weight_month, f"箱の期間{months}か月"))

    if edge_pct is not None and edge_pct <= near_edge_pct:
        weight = float(weights.get("near_edge", 0))
        ratio = 1 - edge_pct / near_edge_pct if near_edge_pct else 1
        score += weight * ratio
        if break_up_pct is not None and break_down_pct is not None:
            if break_up_pct <= break_down_pct:
                reasons.append((weight, f"上抜けまで{break_up_pct:.1f}%"))
            else:
                reasons.append((weight, f"下抜けまで{break_down_pct:.1f}%"))

    if box["wild"] and wild_penalty:
        score += wild_penalty
        reasons.append((wild_penalty, "荒れ箱"))

    closes = [float(row[4]) for row in rows if len(row) >= 5 and row[4] is not None]
    ma7_series = _build_ma_series(closes, 7)
    ma20_series = _build_ma_series(closes, 20)
    ma60_series = _build_ma_series(closes, 60)
    ma7 = ma7_series[-1] if ma7_series else None
    ma20 = ma20_series[-1] if ma20_series else None
    ma60 = ma60_series[-1] if ma60_series else None

    reasons.sort(key=lambda item: item[0], reverse=True)

    levels = {
        "close": close,
        "ma7": ma7,
        "ma20": ma20,
        "ma60": ma60,
        "atr14": None
    }

    chart_hint = {
        "lines": {
            "box_upper": upper,
            "box_lower": lower,
            "ma20": ma20
        }
    }

    return {
        "code": code,
        "name": name or code,
        "as_of": _format_month_label(box["end"]),
        "total_score": round(score, 3),
        "reasons": [label for _, label in reasons[:max_reasons]],
        "levels": levels,
        "distance_to_trigger": {
            "break_up_pct": break_up_pct,
            "break_down_pct": break_down_pct
        },
        "box_info": {
            "box_start": _format_month_label(box["start"]),
            "box_end": _format_month_label(box["end"]),
            "box_upper_body": upper,
            "box_lower_body": lower,
            "box_months": months,
            "wild_box_flag": box["wild"],
            "range_pct": box["range_pct"]
        },
        "box_start": _format_month_label(box["start"]),
        "box_end": _format_month_label(box["end"]),
        "box_upper_body": upper,
        "box_lower_body": lower,
        "box_months": months,
        "wild_box_flag": box["wild"],
        "chart_hint": chart_hint
    }, None


def _rank_cache_key(as_of: str | None, limit: int, universe_meta: dict) -> str:
    uni_key = universe_meta.get("path") or universe_meta.get("requested") or "all"
    mtime = universe_meta.get("mtime")
    return f"{as_of or 'latest'}|{limit}|{uni_key}|{mtime or 'none'}"


def _ensure_rank_cache_state() -> tuple[float | None, float | None]:
    db_mtime = os.path.getmtime(DEFAULT_DB_PATH) if os.path.isfile(DEFAULT_DB_PATH) else None
    config_mtime = _rank_config_cache.get("mtime")
    if _rank_cache.get("mtime") != db_mtime or _rank_cache.get("config_mtime") != config_mtime:
        _rank_cache["weekly"] = {}
        _rank_cache["monthly"] = {}
        _rank_cache["mtime"] = db_mtime
        _rank_cache["config_mtime"] = config_mtime
    return db_mtime, config_mtime


def _build_weekly_ranking(as_of: str | None, limit: int, universe: str | None) -> dict:
    start = time.perf_counter()
    config = _load_rank_config()
    _ensure_rank_cache_state()
    as_of_int = _as_of_int(as_of)
    common = _get_config_value(config, ["common"], {})
    max_bars = int(common.get("max_daily_bars", 260))

    with get_conn() as conn:
        codes, universe_meta = _resolve_universe_codes(conn, universe)
        if not codes:
            return {"up": [], "down": [], "meta": {"as_of": as_of, "count": 0, "errors": []}}
        cache_key = _rank_cache_key(as_of, limit, universe_meta)
        cached = _rank_cache["weekly"].get(cache_key)
        if cached:
            return cached
        meta_rows = conn.execute(
            f"SELECT code, name FROM stock_meta WHERE code IN ({','.join(['?'] * len(codes))})",
            codes
        ).fetchall()
        name_map = {row[0]: row[1] for row in meta_rows}
        daily_map = _fetch_daily_rows(conn, codes, as_of_int, max_bars)

    up_items: list[dict] = []
    down_items: list[dict] = []
    skipped: list[dict] = []

    for code in codes:
        rows = daily_map.get(code, [])
        up_item, down_item, skip_reason = _score_weekly_candidate(code, name_map.get(code, code), rows, config, as_of_int)
        if skip_reason:
            skipped.append({"code": code, "reason": skip_reason})
            continue
        if up_item:
            up_items.append(up_item)
        if down_item:
            down_items.append(down_item)

    up_items.sort(key=lambda item: item.get("total_score", 0), reverse=True)
    down_items.sort(key=lambda item: item.get("total_score", 0), reverse=True)

    elapsed = (time.perf_counter() - start) * 1000
    print(f"[rank_weekly] codes={len(codes)} skipped={len(skipped)} ms={elapsed:.1f}")

    result = {
        "up": up_items[:limit],
        "down": down_items[:limit],
        "meta": {
            "as_of": as_of,
            "count": len(codes),
            "skipped": skipped,
            "elapsed_ms": round(elapsed, 2),
            "universe": universe_meta,
            "errors": []
        }
    }
    _rank_cache["weekly"][cache_key] = result
    return result


def _build_monthly_ranking(as_of: str | None, limit: int, universe: str | None) -> dict:
    start = time.perf_counter()
    config = _load_rank_config()
    _ensure_rank_cache_state()
    as_of_month = _as_of_month_int(as_of)
    common = _get_config_value(config, ["common"], {})
    max_bars = int(common.get("max_monthly_bars", 120))

    with get_conn() as conn:
        codes, universe_meta = _resolve_universe_codes(conn, universe)
        if not codes:
            return {"box": [], "meta": {"as_of": as_of, "count": 0, "errors": []}}
        cache_key = _rank_cache_key(as_of, limit, universe_meta)
        cached = _rank_cache["monthly"].get(cache_key)
        if cached:
            return cached
        meta_rows = conn.execute(
            f"SELECT code, name FROM stock_meta WHERE code IN ({','.join(['?'] * len(codes))})",
            codes
        ).fetchall()
        name_map = {row[0]: row[1] for row in meta_rows}
        monthly_map = _fetch_monthly_rows(conn, codes, as_of_month, max_bars)

    items: list[dict] = []
    skipped: list[dict] = []

    for code in codes:
        rows = monthly_map.get(code, [])
        item, skip_reason = _score_monthly_candidate(code, name_map.get(code, code), rows, config, as_of_month)
        if skip_reason:
            skipped.append({"code": code, "reason": skip_reason})
            continue
        if item:
            items.append(item)

    items.sort(key=lambda item: item.get("total_score", 0), reverse=True)
    elapsed = (time.perf_counter() - start) * 1000
    print(f"[rank_monthly] codes={len(codes)} skipped={len(skipped)} ms={elapsed:.1f}")

    result = {
        "box": items[:limit],
        "meta": {
            "as_of": as_of,
            "count": len(codes),
            "skipped": skipped,
            "elapsed_ms": round(elapsed, 2),
            "universe": universe_meta,
            "errors": []
        }
    }
    _rank_cache["monthly"][cache_key] = result
    return result


def _compute_screener_metrics(
    daily_rows: list[tuple],
    monthly_rows: list[tuple]
) -> dict:
    reasons: list[str] = []
    daily_rows = sorted(daily_rows, key=lambda item: item[0])
    monthly_rows = sorted(monthly_rows, key=lambda item: item[0])

    last_daily = _parse_daily_date(daily_rows[-1][0]) if daily_rows else None
    closes = [float(row[4]) for row in daily_rows if len(row) >= 5 and row[4] is not None]
    last_close = closes[-1] if closes else None
    if last_close is None:
        reasons.append("missing_last_close")

    chg1d = _pct_change(closes[-1], closes[-2]) if len(closes) >= 2 else None

    weekly = _build_weekly_bars(daily_rows)
    weekly = _drop_incomplete_weekly(weekly, last_daily)
    weekly_closes = [item["c"] for item in weekly]
    chg1w = _pct_change(weekly_closes[-1], weekly_closes[-2]) if len(weekly_closes) >= 2 else None
    prev_week_chg = _pct_change(weekly_closes[-2], weekly_closes[-3]) if len(weekly_closes) >= 3 else None

    confirmed_monthly = _drop_incomplete_monthly(monthly_rows, last_daily)
    monthly_closes = [float(row[4]) for row in confirmed_monthly if len(row) >= 5 and row[4] is not None]
    chg1m = _pct_change(monthly_closes[-1], monthly_closes[-2]) if len(monthly_closes) >= 2 else None
    prev_month_chg = _pct_change(monthly_closes[-2], monthly_closes[-3]) if len(monthly_closes) >= 3 else None

    quarterly = _build_quarterly_bars(confirmed_monthly)
    quarterly_closes = [item["c"] for item in quarterly]
    chg1q = _pct_change(quarterly_closes[-1], quarterly_closes[-2]) if len(quarterly_closes) >= 2 else None
    prev_quarter_chg = _pct_change(quarterly_closes[-2], quarterly_closes[-3]) if len(quarterly_closes) >= 3 else None

    yearly = _build_yearly_bars(confirmed_monthly)
    yearly_closes = [item["c"] for item in yearly]
    chg1y = _pct_change(yearly_closes[-1], yearly_closes[-2]) if len(yearly_closes) >= 2 else None
    prev_year_chg = _pct_change(yearly_closes[-2], yearly_closes[-3]) if len(yearly_closes) >= 3 else None

    ma7_series = _build_ma_series(closes, 7)
    ma20_series = _build_ma_series(closes, 20)
    ma60_series = _build_ma_series(closes, 60)
    ma100_series = _build_ma_series(closes, 100)

    ma7 = ma7_series[-1] if ma7_series else None
    ma20 = ma20_series[-1] if ma20_series else None
    ma60 = ma60_series[-1] if ma60_series else None
    ma100 = ma100_series[-1] if ma100_series else None

    prev_ma20 = ma20_series[-2] if len(ma20_series) >= 2 else None
    slope20 = ma20 - prev_ma20 if ma20 is not None and prev_ma20 is not None else None

    up7 = _count_streak(closes, ma7_series, "up")
    down7 = _count_streak(closes, ma7_series, "down")
    up20 = _count_streak(closes, ma20_series, "up")
    down20 = _count_streak(closes, ma20_series, "down")
    up60 = _count_streak(closes, ma60_series, "up")
    down60 = _count_streak(closes, ma60_series, "down")
    up100 = _count_streak(closes, ma100_series, "up")
    down100 = _count_streak(closes, ma100_series, "down")

    if ma20 is None:
        reasons.append("missing_ma20")
    if ma60 is None:
        reasons.append("missing_ma60")
    if ma100 is None:
        reasons.append("missing_ma100")
    if chg1m is None:
        reasons.append("missing_chg1m")
    if chg1q is None:
        reasons.append("missing_chg1q")
    if chg1y is None:
        reasons.append("missing_chg1y")

    box_monthly, box_state, box_end_month, breakout_month, box_direction = _build_box_metrics(
        monthly_rows, last_close
    )

    latest_month_label = _format_month_label(confirmed_monthly[-1][0]) if confirmed_monthly else None
    prev_month_label = _format_month_label(confirmed_monthly[-2][0]) if len(confirmed_monthly) >= 2 else None
    latest_month_value = _month_label_to_int(latest_month_label)
    prev_month_value = _month_label_to_int(prev_month_label)
    box_active = False
    if box_monthly:
        box_start_value = _month_label_to_int(box_monthly.get("startDate"))
        box_end_value = _month_label_to_int(box_monthly.get("endDate"))
        if box_start_value is not None and box_end_value is not None:
            if latest_month_value is not None and box_start_value <= latest_month_value <= box_end_value:
                box_active = True
            elif prev_month_value is not None and box_start_value <= prev_month_value <= box_end_value:
                box_active = True

    monthly_ma7_series = _build_ma_series(monthly_closes, 7)
    monthly_ma20_series = _build_ma_series(monthly_closes, 20)
    monthly_down20 = _count_streak(monthly_closes, monthly_ma20_series, "down")
    bottom_zone = bool(monthly_down20 is not None and monthly_down20 >= 6)

    weekly_closes = [item["c"] for item in weekly]
    weekly_highs = [item["h"] for item in weekly]
    weekly_lows = [item["l"] for item in weekly]
    weekly_ma7_series = _build_ma_series(weekly_closes, 7)
    weekly_ma20_series = _build_ma_series(weekly_closes, 20)
    weekly_ma7 = weekly_ma7_series[-1] if weekly_ma7_series else None
    weekly_ma20 = weekly_ma20_series[-1] if weekly_ma20_series else None
    weekly_above_ma7 = (
        weekly_closes[-1] > weekly_ma7 if weekly_ma7 is not None and weekly_closes else False
    )
    weekly_above_ma20 = (
        weekly_closes[-1] > weekly_ma20 if weekly_ma20 is not None and weekly_closes else False
    )

    weekly_low_stop = False
    if len(weekly_lows) >= 6:
        recent_lows = weekly_lows[-6:]
        previous_lows = weekly_lows[:-6]
        if previous_lows:
            weekly_low_stop = min(recent_lows) >= min(previous_lows)

    weekly_range_contraction = False
    if len(weekly_highs) >= 12:
        recent_range = max(weekly_highs[-6:]) - min(weekly_lows[-6:])
        prev_range = max(weekly_highs[-12:-6]) - min(weekly_lows[-12:-6])
        if prev_range > 0 and recent_range <= prev_range * 0.8:
            weekly_range_contraction = True

    daily_cross_ma7 = False
    daily_cross_ma20 = False
    if len(closes) >= 2 and len(ma7_series) >= 2:
        daily_cross_ma7 = closes[-1] > ma7_series[-1] and closes[-2] <= ma7_series[-2]
    if len(closes) >= 2 and len(ma20_series) >= 2:
        daily_cross_ma20 = closes[-1] > ma20_series[-1] and closes[-2] <= ma20_series[-2]

    daily_pre_signal = False
    if daily_rows:
        last_row = daily_rows[-1]
        if len(last_row) >= 5:
            open_ = float(last_row[1]) if last_row[1] is not None else None
            high = float(last_row[2]) if last_row[2] is not None else None
            low = float(last_row[3]) if last_row[3] is not None else None
            close = float(last_row[4]) if last_row[4] is not None else None
            if open_ is not None and high is not None and low is not None and close is not None:
                rng = max(high - low, 1e-9)
                body = abs(close - open_)
                lower_shadow = min(open_, close) - low
                if body / rng <= 0.35 or lower_shadow / rng >= 0.45:
                    daily_pre_signal = True

    daily_low_break = False
    if len(daily_rows) >= 11:
        lows = [
            float(row[3])
            for row in daily_rows[-11:-1]
            if len(row) >= 4 and row[3] is not None
        ]
        if lows and daily_rows[-1][3] is not None:
            daily_low_break = float(daily_rows[-1][3]) < min(lows)

    weekly_low_break = False
    if len(weekly_lows) >= 7:
        weekly_low_break = weekly_lows[-1] < min(weekly_lows[-7:-1])

    falling_knife = daily_low_break or weekly_low_break
    monthly_ok = box_active or bottom_zone

    score_monthly = 0
    if box_active:
        score_monthly += 18
    if bottom_zone:
        score_monthly += 12

    score_weekly = 0
    if weekly_low_stop:
        score_weekly += 15
    if weekly_range_contraction:
        score_weekly += 10
    if weekly_above_ma7:
        score_weekly += 7
    if weekly_above_ma20:
        score_weekly += 8

    score_daily = 0
    if daily_cross_ma7:
        score_daily += 10
    if daily_cross_ma20:
        score_daily += 12
    if daily_pre_signal:
        score_daily += 8

    daily_ma20_down = False
    if len(ma20_series) >= 2:
        daily_ma20_down = ma20_series[-1] < ma20_series[-2]

    buy_state = "その他"
    buy_state_rank = 0
    buy_state_score = 0
    buy_state_reason_parts: list[str] = []

    if monthly_ok and weekly_low_stop and not falling_knife:
        if daily_cross_ma7 or daily_cross_ma20 or daily_pre_signal:
            buy_state = "初動"
            buy_state_rank = 2
            buy_state_score = score_monthly + score_weekly + score_daily
            if daily_ma20_down and ma20 is not None and last_close is not None and last_close < ma20:
                buy_state_score -= 15
        elif weekly_range_contraction:
            buy_state = "底がため"
            buy_state_rank = 1
            buy_state_score = score_monthly + score_weekly + min(score_daily, 10)

    if buy_state_score < 0:
        buy_state_score = 0
    if buy_state == "初動":
        buy_state_score = min(100, buy_state_score)
    elif buy_state == "底がため":
        buy_state_score = min(80, buy_state_score)

    if monthly_ok:
        month_parts = []
        if box_active:
            month_parts.append("箱有")
        if bottom_zone:
            month_parts.append("大底警戒")
        buy_state_reason_parts.append(f"月:{'/'.join(month_parts)}")
    if weekly_low_stop or weekly_range_contraction:
        week_parts = []
        if weekly_low_stop:
            week_parts.append("安値更新停止")
        if weekly_range_contraction:
            week_parts.append("収縮")
        if weekly_above_ma7:
            week_parts.append("7MA上")
        if weekly_above_ma20:
            week_parts.append("20MA上")
        buy_state_reason_parts.append(f"週:{'/'.join(week_parts)}")
    if daily_cross_ma7 or daily_cross_ma20 or daily_pre_signal:
        day_parts = []
        if daily_cross_ma7:
            day_parts.append("7MA上抜け")
        if daily_cross_ma20:
            day_parts.append("20MA上抜け")
        if daily_pre_signal:
            day_parts.append("事前決定打")
        buy_state_reason_parts.append(f"日:{'/'.join(day_parts)}")
    if falling_knife:
        buy_state_reason_parts.append("落ちるナイフ")

    buy_state_reason = " / ".join(buy_state_reason_parts) if buy_state_reason_parts else "N/A"

    buy_risk_distance = None
    if last_close is not None and box_monthly and box_monthly.get("bodyLow") is not None:
        body_low = float(box_monthly["bodyLow"])
        if last_close > 0:
            buy_risk_distance = max(0.0, (last_close - body_low) / last_close * 100)

    status_label = "UNKNOWN"
    essential_missing = last_close is None or ma20 is None or ma60 is None
    if not essential_missing:
        if last_close > ma20 and ma20 > ma60:
            status_label = "UP"
        elif last_close < ma20 and ma20 < ma60:
            status_label = "DOWN"
        else:
            status_label = "RANGE"

    up_score = None
    down_score = None
    overheat_up = None
    overheat_down = None

    if status_label != "UNKNOWN" and last_close is not None and ma20 is not None and ma60 is not None:
        up_score = 0
        down_score = 0

        if last_close > ma20:
            up_score += 10
        if ma20 > ma60:
            up_score += 10
        if slope20 is not None and slope20 > 0:
            up_score += 10

        if up7 is not None:
            if up7 >= 14:
                up_score += 20
            elif up7 >= 7:
                up_score += 10

        if box_state != "NONE":
            if box_direction == "BREAKOUT_UP":
                up_score += 30
            elif box_state == "IN_BOX" and box_monthly and box_monthly.get("months", 0) >= 3:
                up_score += 10

        if chg1m is not None and chg1m > 0:
            up_score += 10
        if chg1q is not None and chg1q > 0:
            up_score += 10

        if last_close < ma20:
            down_score += 10
        if ma20 < ma60:
            down_score += 10
        if slope20 is not None and slope20 < 0:
            down_score += 10

        if down7 is not None:
            if down7 >= 14:
                down_score += 20
            elif down7 >= 7:
                down_score += 10

        if box_state != "NONE" and box_direction == "BREAKOUT_DOWN":
            down_score += 30

        if chg1m is not None and chg1m < 0:
            down_score += 10
        if chg1q is not None and chg1q < 0:
            down_score += 10

        up_score = min(100, max(0, up_score))
        down_score = min(100, max(0, down_score))

        if up20 is not None:
            overheat_up = min(1.0, max(0.0, (up20 - 16) / 4))
        if down20 is not None:
            overheat_down = min(1.0, max(0.0, (down20 - 16) / 4))

    return {
        "lastClose": last_close,
        "chg1D": chg1d,
        "chg1W": chg1w,
        "chg1M": chg1m,
        "chg1Q": chg1q,
        "chg1Y": chg1y,
        "prevWeekChg": prev_week_chg,
        "prevMonthChg": prev_month_chg,
        "prevQuarterChg": prev_quarter_chg,
        "prevYearChg": prev_year_chg,
        "ma7": ma7,
        "ma20": ma20,
        "ma60": ma60,
        "ma100": ma100,
        "slope20": slope20,
        "counts": {
            "up7": up7,
            "down7": down7,
            "up20": up20,
            "down20": down20,
            "up60": up60,
            "down60": down60,
            "up100": up100,
            "down100": down100
        },
        "boxMonthly": box_monthly,
        "boxState": box_state,
        "boxEndMonth": box_end_month,
        "breakoutMonth": breakout_month,
        "boxActive": box_active,
        "hasBox": box_active,
        "box_state": box_state,
        "box_end_month": box_end_month,
        "breakout_month": breakout_month,
        "box_active": box_active,
        "buyState": buy_state,
        "buyStateRank": buy_state_rank,
        "buyStateScore": buy_state_score,
        "buyStateReason": buy_state_reason,
        "buyRiskDistance": buy_risk_distance,
        "buy_state": buy_state,
        "buy_state_rank": buy_state_rank,
        "buy_state_score": buy_state_score,
        "buy_state_reason": buy_state_reason,
        "buy_risk_distance": buy_risk_distance,
        "buyStateDetails": {
            "monthly": score_monthly,
            "weekly": score_weekly,
            "daily": score_daily
        },
        "scores": {
            "upScore": up_score,
            "downScore": down_score,
            "overheatUp": overheat_up,
            "overheatDown": overheat_down
        },
        "statusLabel": status_label,
        "reasons": reasons
    }


def _build_screener_rows() -> list[dict]:
    with get_conn() as conn:
        codes = [row[0] for row in conn.execute("SELECT DISTINCT code FROM daily_bars ORDER BY code").fetchall()]
        meta_rows = conn.execute(
            "SELECT code, name, stage, score, reason, score_status, missing_reasons_json, score_breakdown_json FROM stock_meta"
        ).fetchall()
        daily_rows = conn.execute(
            """
            SELECT code, date, o, h, l, c, v
            FROM (
                SELECT
                    code,
                    date,
                    o,
                    h,
                    l,
                    c,
                    v,
                    ROW_NUMBER() OVER (PARTITION BY code ORDER BY date DESC) AS rn
                FROM daily_bars
            )
            WHERE rn <= 260
            ORDER BY code, date
            """
        ).fetchall()
        monthly_rows = conn.execute(
            """
            SELECT code, month, o, h, l, c
            FROM monthly_bars
            ORDER BY code, month
            """
        ).fetchall()

    meta_map = {row[0]: row for row in meta_rows}
    fallback_names = _build_name_map_from_txt()
    daily_map: dict[str, list[tuple]] = {}
    monthly_map: dict[str, list[tuple]] = {}

    for row in daily_rows:
        code = row[0]
        daily_map.setdefault(code, []).append(row[1:])

    for row in monthly_rows:
        code = row[0]
        monthly_map.setdefault(code, []).append(row[1:])

    items: list[dict] = []
    for code in codes:
        meta = meta_map.get(code)
        name = meta[1] if meta else None
        stage = meta[2] if meta else None
        score = meta[3] if meta and meta[3] is not None else None
        reason = meta[4] if meta and meta[4] is not None else ""
        score_status = meta[5] if meta else None
        missing_reasons = []
        if meta and meta[6]:
            try:
                missing_reasons = json.loads(meta[6]) or []
            except (TypeError, json.JSONDecodeError):
                missing_reasons = []
        score_breakdown = None
        if meta and meta[7]:
            try:
                score_breakdown = json.loads(meta[7]) or None
            except (TypeError, json.JSONDecodeError):
                score_breakdown = None
        metrics = _compute_screener_metrics(daily_map.get(code, []), monthly_map.get(code, []))
        fallback_name = fallback_names.get(code)
        if not name or name == code:
            name = fallback_name
        if not name:
            name = code
        if not stage or stage.upper() == "UNKNOWN":
            stage = metrics.get("statusLabel") or stage or "UNKNOWN"
        if isinstance(score, (int, float)) and float(score) == 0.0:
            if (
                not score_status
                or score_status == "INSUFFICIENT_DATA"
                or not reason
                or reason == "TODO"
                or not stage
                or (isinstance(stage, str) and stage.upper() == "UNKNOWN")
            ):
                score = None
                score_status = "INSUFFICIENT_DATA"
        if score is None:
            fallback_score = None
            buy_score = metrics.get("buyStateScore")
            if isinstance(buy_score, (int, float)) and buy_score > 0:
                fallback_score = float(buy_score)
            else:
                scores = metrics.get("scores") or {}
                if isinstance(scores, dict):
                    values = [
                        scores.get("upScore"),
                        scores.get("downScore")
                    ]
                    values = [float(v) for v in values if isinstance(v, (int, float)) and v > 0]
                    if values:
                        fallback_score = max(values)
            if fallback_score is not None:
                score = fallback_score
                if not reason:
                    reason = "DERIVED"
                if not score_status:
                    score_status = "OK"
        if not score_status:
            score_status = "OK" if score is not None else "INSUFFICIENT_DATA"
        if not missing_reasons:
            missing_reasons = metrics.get("reasons") or []
        items.append(
            {
                "code": code,
                "name": name,
                "stage": stage,
                "score": score,
                "reason": reason,
                "scoreStatus": score_status,
                "score_status": score_status,
                "missingReasons": missing_reasons,
                "missing_reasons": missing_reasons,
                "scoreBreakdown": score_breakdown,
                "score_breakdown": score_breakdown,
                **metrics
            }
        )
    return items


def _get_screener_rows() -> list[dict]:
    mtime = None
    if os.path.isfile(DEFAULT_DB_PATH):
        mtime = os.path.getmtime(DEFAULT_DB_PATH)
    if _screener_cache["mtime"] == mtime and _screener_cache["rows"]:
        return _screener_cache["rows"]

    rows = _build_screener_rows()
    _screener_cache["mtime"] = mtime
    _screener_cache["rows"] = rows
    return rows


def get_txt_status() -> dict:
    if not os.path.isdir(PAN_OUT_TXT_DIR):
        return {
            "txt_count": 0,
            "code_txt_missing": False,
            "last_updated": None
        }

    txt_files = [
        os.path.join(PAN_OUT_TXT_DIR, name)
        for name in os.listdir(PAN_OUT_TXT_DIR)
        if name.endswith(".txt") and name.lower() != "code.txt"
    ]
    code_txt_missing = False
    if USE_CODE_TXT:
        code_txt_missing = find_code_txt_path(PAN_OUT_TXT_DIR) is None
    last_updated = None
    if txt_files:
        last_updated = max(os.path.getmtime(path) for path in txt_files)
        last_updated = datetime.utcfromtimestamp(last_updated).isoformat() + "Z"

    return {
        "txt_count": len(txt_files),
        "code_txt_missing": code_txt_missing,
        "last_updated": last_updated
    }


def _run_command(cmd: list[str], timeout: int) -> tuple[int, str]:
    creationflags = subprocess.CREATE_NO_WINDOW if os.name == "nt" else 0
    result = subprocess.run(
        cmd,
        capture_output=True,
        text=True,
        timeout=timeout,
        creationflags=creationflags
    )
    output = "\n".join([result.stdout or "", result.stderr or ""]).strip()
    if len(output) > 8000:
        output = output[-8000:]
    return result.returncode, output


def _read_text_lines(path: str) -> list[str]:
    for encoding in ("utf-8", "cp932"):
        try:
            with open(path, "r", encoding=encoding, errors="ignore") as handle:
                return handle.read().splitlines()
        except OSError:
            break
    return []


def _count_codes(path: str) -> int:
    count = 0
    for line in _read_text_lines(path):
        text = line.strip()
        if not text:
            continue
        if text.startswith("#") or text.startswith("'"):
            continue
        count += 1
    return count


def _append_stdout_tail(line: str) -> None:
    with _update_txt_lock:
        tail = list(_update_txt_status.get("stdout_tail") or [])
        tail.append(line)
        if len(tail) > 20:
            tail = tail[-20:]
        _update_txt_status["stdout_tail"] = tail


def _set_update_status(**kwargs) -> None:
    with _update_txt_lock:
        _update_txt_status.update(kwargs)


def _get_update_status_snapshot() -> dict:
    with _update_txt_lock:
        return dict(_update_txt_status)


def _run_streaming_command(cmd: list[str], timeout: int, on_line) -> tuple[int, str, bool]:
    creationflags = subprocess.CREATE_NO_WINDOW if os.name == "nt" else 0
    process = subprocess.Popen(
        cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        text=True,
        encoding="cp932" if os.name == "nt" else "utf-8",
        errors="replace",
        creationflags=creationflags
    )
    output_lines: list[str] = []
    start = time.time()
    timed_out = False
    while True:
        if process.stdout is None:
            break
        line = process.stdout.readline()
        if line:
            text = line.rstrip()
            output_lines.append(text)
            on_line(text)
        if process.poll() is not None:
            break
        if time.time() - start > timeout:
            process.kill()
            timed_out = True
            break
    if process.stdout is not None:
        remaining = process.stdout.read()
        if remaining:
            for extra in remaining.splitlines():
                output_lines.append(extra)
                on_line(extra)
    return process.wait(), "\n".join(output_lines).strip(), timed_out


def _load_update_state() -> dict:
    if not os.path.isfile(UPDATE_STATE_PATH):
        return {}
    try:
        with open(UPDATE_STATE_PATH, "r", encoding="utf-8") as handle:
            return json.load(handle)
    except (OSError, json.JSONDecodeError):
        return {}


def _save_update_state(state: dict) -> None:
    try:
        with open(UPDATE_STATE_PATH, "w", encoding="utf-8") as handle:
            json.dump(state, handle, ensure_ascii=False, indent=2)
    except OSError:
        pass


def _parse_vbs_summary(output: str) -> dict:
    summary: dict[str, int] = {}
    for line in output.splitlines():
        if line.startswith("SUMMARY:"):
            for key, value in re.findall(r"(\\w+)=(\\d+)", line):
                summary[key] = int(value)
    return summary


def _run_txt_update_job(code_path: str, out_dir: str) -> None:
    processed = 0

    def on_line(line: str) -> None:
        nonlocal processed
        _append_stdout_tail(line)
        if line.startswith(("OK   :", "ERROR:", "SPLIT :")):
            processed += 1
            _set_update_status(processed=processed)

    try:
        sys_root = os.environ.get("SystemRoot") or "C:\\Windows"
        cscript = os.path.join(sys_root, "SysWOW64", "cscript.exe")
        if not os.path.isfile(cscript):
            cscript = os.path.join(sys_root, "System32", "cscript.exe")
        vbs_cmd = [cscript, "//nologo", UPDATE_VBS_PATH, code_path, out_dir]
        timeout_sec = 1800
        vbs_code, vbs_output, timed_out = _run_streaming_command(
            vbs_cmd, timeout=timeout_sec, on_line=on_line
        )
        summary = _parse_vbs_summary(vbs_output)
        _set_update_status(summary=summary)
        if timed_out:
            _set_update_status(
                running=False,
                phase="error",
                error="timeout",
                finished_at=datetime.now().isoformat(),
                timeout_sec=timeout_sec
            )
            return
        if vbs_code != 0:
            _set_update_status(
                running=False,
                phase="error",
                error=f"vbs_failed:{vbs_code}",
                finished_at=datetime.now().isoformat()
            )
            return

        _set_update_status(phase="ingesting")
        ingest_code, ingest_output = _run_command([sys.executable, INGEST_SCRIPT_PATH], timeout=3600)
        for line in ingest_output.splitlines():
            _append_stdout_tail(line)
        if ingest_code != 0:
            _set_update_status(
                running=False,
                phase="error",
                error=f"ingest_failed:{ingest_code}",
                finished_at=datetime.now().isoformat(),
                summary=summary
            )
            return

        state = _load_update_state()
        state["last_txt_update_date"] = datetime.now().date().isoformat()
        state["last_txt_update_at"] = datetime.now().isoformat()
        _save_update_state(state)
        _set_update_status(
            running=False,
            phase="done",
            error=None,
            finished_at=datetime.now().isoformat(),
            summary=summary,
            last_updated_at=state.get("last_txt_update_at"),
            processed=processed
        )
    except Exception as exc:
        _append_stdout_tail(str(exc))
        _set_update_status(
            running=False,
            phase="error",
            error=f"update_txt_failed:{exc}",
            finished_at=datetime.now().isoformat()
        )


def _start_txt_update(code_path: str, out_dir: str, total: int, cscript: str) -> dict:
    started_at = datetime.now().isoformat()
    with _update_txt_lock:
        if _update_txt_status.get("running"):
            return {}
        _update_txt_status.update(
            {
                "running": True,
                "phase": "running",
                "started_at": started_at,
                "finished_at": None,
                "processed": 0,
                "total": total,
                "summary": {},
                "error": None,
                "stdout_tail": [],
                "code_path": code_path,
                "out_dir": out_dir,
                "script_path": UPDATE_VBS_PATH,
                "cscript_path": cscript
            }
        )
    thread = threading.Thread(target=_run_txt_update_job, args=(code_path, out_dir), daemon=True)
    thread.start()
    return {"ok": True, "started": True, "started_at": started_at, "total": total}


@app.post("/api/txt_update/run")
def txt_update_run():
    state = _load_update_state()
    today = datetime.now().date().isoformat()
    if state.get("last_txt_update_date") == today:
        return JSONResponse(
            status_code=200,
            content={
                "ok": False,
                "error": "already_updated_today",
                "last_updated_at": state.get("last_txt_update_at")
            }
        )

    if not os.path.isfile(UPDATE_VBS_PATH):
        return JSONResponse(
            status_code=404,
            content={"ok": False, "error": f"vbs_not_found:{UPDATE_VBS_PATH}"}
        )

    if not os.path.isfile(INGEST_SCRIPT_PATH):
        return JSONResponse(
            status_code=404,
            content={"ok": False, "error": f"ingest_not_found:{INGEST_SCRIPT_PATH}"}
        )

    code_path = PAN_CODE_TXT_PATH if os.path.isfile(PAN_CODE_TXT_PATH) else None
    if not code_path:
        return JSONResponse(
            status_code=400,
            content={
                "ok": False,
                "error": "code_txt_missing",
                "searched": [PAN_CODE_TXT_PATH]
            }
        )

    os.makedirs(PAN_OUT_TXT_DIR, exist_ok=True)
    total = _count_codes(code_path)
    sys_root = os.environ.get("SystemRoot") or "C:\\Windows"
    cscript = os.path.join(sys_root, "SysWOW64", "cscript.exe")
    if not os.path.isfile(cscript):
        cscript = os.path.join(sys_root, "System32", "cscript.exe")
    started = _start_txt_update(code_path, PAN_OUT_TXT_DIR, total, cscript)
    if not started:
        return JSONResponse(status_code=409, content={"ok": False, "error": "update_in_progress"})
    return started


@app.get("/api/txt_update/status")
def txt_update_status():
    snapshot = _get_update_status_snapshot()
    if not snapshot.get("last_updated_at"):
        state = _load_update_state()
        snapshot["last_updated_at"] = state.get("last_txt_update_at")
    summary = snapshot.get("summary") or {}
    if summary.get("ok", 0) > 0 and summary.get("err", 0) > 0:
        snapshot["warning"] = True
    else:
        snapshot["warning"] = False
    elapsed_ms = None
    if snapshot.get("started_at"):
        try:
            started = datetime.fromisoformat(snapshot["started_at"])
            elapsed_ms = int((datetime.now() - started).total_seconds() * 1000)
        except ValueError:
            elapsed_ms = None
    snapshot["elapsed_ms"] = elapsed_ms
    return snapshot


@app.get("/api/txt_update/split_suspects")
def txt_update_split_suspects():
    if not os.path.isfile(SPLIT_SUSPECTS_PATH):
        return {"items": []}
    items = []
    try:
        for line in _read_text_lines(SPLIT_SUSPECTS_PATH):
            if not line or line.lower().startswith("code,"):
                continue
            parts = [part.strip() for part in line.split(",")]
            if len(parts) < 7:
                continue
            items.append(
                {
                    "code": parts[0],
                    "file_date": parts[1],
                    "file_close": parts[2],
                    "pan_date": parts[3],
                    "pan_close": parts[4],
                    "diff_ratio": parts[5],
                    "reason": parts[6],
                    "detected_at": parts[7] if len(parts) > 7 else ""
                }
            )
        return {"items": items}
    except Exception as exc:
        return JSONResponse(status_code=200, content={"items": [], "error": str(exc)})


@app.post("/api/update_txt")
def update_txt():
    return txt_update_run()


@app.get("/api/watchlist")
def get_watchlist():
    path = PAN_CODE_TXT_PATH
    if not os.path.isfile(path):
        return {"codes": [], "path": path, "missing": True}
    with _watchlist_lock:
        codes = _load_watchlist_codes(path)
    return {"codes": codes, "path": path, "missing": False}


@app.post("/api/watchlist/add")
def watchlist_add(payload: dict = Body(default=None)):
    payload = payload or {}
    code = _normalize_watch_code(payload.get("code"))
    if not code:
        return JSONResponse(status_code=400, content={"ok": False, "error": "invalid_code"})
    path = PAN_CODE_TXT_PATH
    with _watchlist_lock:
        codes = _load_watchlist_codes(path) if os.path.isfile(path) else []
        already = code in codes
        _update_watchlist_file(path, code, remove=False)
    return {"ok": True, "code": code, "alreadyExisted": already}


@app.post("/api/watchlist/remove")
def watchlist_remove(payload: dict = Body(default=None)):
    payload = payload or {}
    code = _normalize_watch_code(payload.get("code"))
    if not code:
        return JSONResponse(status_code=400, content={"ok": False, "error": "invalid_code"})
    delete_artifacts = payload.get("deleteArtifacts", True)
    path = PAN_CODE_TXT_PATH
    if not os.path.isfile(path):
        return JSONResponse(status_code=400, content={"ok": False, "error": "code_txt_missing"})
    with _watchlist_lock:
        removed = _update_watchlist_file(path, code, remove=True)
        trash_token = None
        trashed: list[str] = []
        if delete_artifacts:
            trash_token, trashed = _trash_watchlist_artifacts(code)
    return {
        "ok": True,
        "code": code,
        "removed": removed,
        "deleteArtifacts": bool(delete_artifacts),
        "trashed": trashed,
        "trashToken": trash_token
    }


@app.post("/api/watchlist/undo_remove")
def watchlist_undo_remove(payload: dict = Body(default=None)):
    payload = payload or {}
    code = _normalize_watch_code(payload.get("code"))
    token = payload.get("trashToken") or ""
    if not code:
        return JSONResponse(status_code=400, content={"ok": False, "error": "invalid_code"})
    with _watchlist_lock:
        restored = _restore_watchlist_artifacts(token)
        _update_watchlist_file(PAN_CODE_TXT_PATH, code, remove=False)
    return {"ok": True, "code": code, "restored": restored}


def _list_tables(conn) -> set[str]:
    rows = conn.execute("SELECT table_name FROM duckdb_tables()").fetchall()
    return {row[0] for row in rows}


def _collect_db_stats() -> dict:
    stats = {
        "tickers": None,
        "daily_rows": None,
        "monthly_rows": None,
        "missing_tables": [],
        "errors": []
    }
    required_tables = ["tickers", "daily_bars", "monthly_bars", "daily_ma", "monthly_ma"]
    try:
        with get_conn() as conn:
            tables = _list_tables(conn)
            stats["missing_tables"] = [name for name in required_tables if name not in tables]
            if "tickers" in tables:
                stats["tickers"] = conn.execute("SELECT COUNT(*) FROM tickers").fetchone()[0]
            if "daily_bars" in tables:
                stats["daily_rows"] = conn.execute("SELECT COUNT(*) FROM daily_bars").fetchone()[0]
            if "monthly_bars" in tables:
                stats["monthly_rows"] = conn.execute("SELECT COUNT(*) FROM monthly_bars").fetchone()[0]
    except Exception as exc:
        stats["errors"].append(str(exc))
    return stats


@app.get("/api/health")
def health():
    now = datetime.utcnow().isoformat()
    status = get_txt_status()
    stats = _collect_db_stats()
    is_data_ready = (
        not stats["missing_tables"]
        and stats["errors"] == []
        and (stats["daily_rows"] or 0) > 0
        and (stats["monthly_rows"] or 0) > 0
    )
    if not is_data_ready:
        return JSONResponse(
            status_code=503,
            content={
                "ok": False,
                "status": "starting",
                "ready": False,
                "phase": "starting",
                "message": "起動中",
                "error_code": "DATA_NOT_INITIALIZED",
                "version": APP_VERSION,
                "env": APP_ENV,
                "time": now,
                "retryAfterMs": 1000,
                "stats": stats,
                "txt_count": status.get("txt_count"),
                "last_updated": status.get("last_updated"),
                "code_txt_missing": status.get("code_txt_missing"),
                "errors": stats["errors"] + [f"missing_tables:{','.join(stats['missing_tables'])}"]
                if stats["missing_tables"]
                else stats["errors"]
            }
        )
    return {
        "ok": True,
        "status": "ok",
        "ready": True,
        "phase": "ready",
        "message": "準備完了",
        "version": APP_VERSION,
        "env": APP_ENV,
        "time": now,
        "stats": {
            "tickers": stats["tickers"],
            "daily_rows": stats["daily_rows"],
            "monthly_rows": stats["monthly_rows"]
        },
        "txt_count": status.get("txt_count"),
        "code_count": stats["tickers"],
        "last_updated": status.get("last_updated"),
        "code_txt_missing": status.get("code_txt_missing"),
        "errors": []
    }


@app.get("/api/diagnostics")
def diagnostics():
    now = datetime.utcnow().isoformat()
    db_path = os.path.abspath(DEFAULT_DB_PATH)
    stats = _collect_db_stats()
    return {
        "ok": True,
        "version": APP_VERSION,
        "env": APP_ENV,
        "time": now,
        "data_dir": DATA_DIR,
        "pan_out_txt_dir": PAN_OUT_TXT_DIR,
        "db_path": db_path,
        "db_exists": os.path.isfile(db_path),
        "stats": stats
    }


@app.get("/api/trades/{code}")
def trades_by_code(code: str):
    try:
        parsed = _parse_trade_csv()
        items = [row for row in parsed["rows"] if row.get("code") == code]
        events = [_strip_internal(row) for row in items]
        warnings = _build_warning_payload(parsed["warnings"], code)
        daily_positions = _build_daily_positions(items)
        current_position = daily_positions[-1] if daily_positions else None
        return JSONResponse(
            content={
                "events": events,
                "dailyPositions": daily_positions,
                "currentPosition": current_position,
                "warnings": warnings,
                "errors": []
            }
        )
    except Exception as exc:
        return JSONResponse(
            content={
                "events": [],
                "dailyPositions": [],
                "currentPosition": None,
                "warnings": {"items": []},
                "errors": [f"trades_by_code_failed:{exc}"]
            }
        )


@app.get("/api/trades")
def trades(code: str | None = None):
    try:
        parsed = _parse_trade_csv()
        items = parsed["rows"]
        if code:
            items = [row for row in items if row.get("code") == code]
        warnings = _build_warning_payload(parsed["warnings"], code)
        events = [_strip_internal(row) for row in items]
        daily_positions = _build_daily_positions(items)
        current_position = daily_positions[-1] if daily_positions else None
        return JSONResponse(
            content={
                "events": events,
                "dailyPositions": daily_positions,
                "currentPosition": current_position,
                "warnings": warnings,
                "errors": []
            }
        )
    except Exception as exc:
        return JSONResponse(
            content={
                "events": [],
                "dailyPositions": [],
                "currentPosition": None,
                "warnings": {"items": []},
                "errors": [f"trades_failed:{exc}"]
            }
        )


@app.get("/api/list")
def list_tickers():
    with get_conn() as conn:
        rows = conn.execute(
            """
            SELECT d.code,
                   COALESCE(m.name, d.code) AS name,
                   COALESCE(m.stage, 'UNKNOWN') AS stage,
                   m.score AS score,
                   COALESCE(m.reason, 'TXT_ONLY') AS reason
            FROM (SELECT DISTINCT code FROM daily_bars) d
            LEFT JOIN stock_meta m ON d.code = m.code
            ORDER BY d.code
            """
        ).fetchall()
    return JSONResponse(content=rows)


@app.get("/rank/weekly")
def rank_weekly(as_of: str | None = None, limit: int = 50, universe: str | None = None):
    try:
        limit_value = max(1, min(200, int(limit)))
    except (TypeError, ValueError):
        limit_value = 50
    try:
        result = _build_weekly_ranking(as_of, limit_value, universe)
        return JSONResponse(content=result)
    except Exception as exc:
        return JSONResponse(
            content={
                "up": [],
                "down": [],
                "meta": {
                    "as_of": as_of,
                    "count": 0,
                    "errors": [f"rank_weekly_failed:{exc}"]
                }
            }
        )


@app.get("/rank/monthly")
def rank_monthly(as_of: str | None = None, limit: int = 50, universe: str | None = None):
    try:
        limit_value = max(1, min(200, int(limit)))
    except (TypeError, ValueError):
        limit_value = 50
    try:
        result = _build_monthly_ranking(as_of, limit_value, universe)
        return JSONResponse(content=result)
    except Exception as exc:
        return JSONResponse(
            content={
                "box": [],
                "meta": {
                    "as_of": as_of,
                    "count": 0,
                    "errors": [f"rank_monthly_failed:{exc}"]
                }
            }
        )


@app.get("/rank")
@app.get("/api/rank")
def rank_dir(dir: str = "up", as_of: str | None = None, limit: int = 50, universe: str | None = None):
    try:
        limit_value = max(1, min(200, int(limit)))
    except (TypeError, ValueError):
        limit_value = 50
    direction = (dir or "up").lower()
    if direction not in ("up", "down"):
        direction = "up"
    try:
        result = _build_weekly_ranking(as_of, limit_value, universe)
        favorites = set(_load_favorite_codes())
        items = []
        for item in result.get(direction, []):
            code = item.get("code")
            items.append(
                {
                    **item,
                    "is_favorite": bool(code and code in favorites)
                }
            )
        return JSONResponse(
            content={
                "items": items,
                "meta": {
                    "as_of": result.get("meta", {}).get("as_of"),
                    "count": len(items),
                    "dir": direction,
                    "universe": result.get("meta", {}).get("universe")
                },
                "errors": []
            }
        )
    except Exception as exc:
        return JSONResponse(
            content={
                "items": [],
                "meta": {"as_of": as_of, "count": 0, "dir": direction, "universe": universe},
                "errors": [f"rank_failed:{exc}"]
            }
        )


@app.get("/favorites")
@app.get("/api/favorites")
def favorites_list():
    try:
        items = _load_favorite_items()
        return JSONResponse(content={"items": items, "errors": []})
    except Exception as exc:
        return JSONResponse(content={"items": [], "errors": [f"favorites_failed:{exc}"]})


@app.post("/favorites/{code}")
@app.post("/api/favorites/{code}")
def favorites_add(code: str):
    normalized = _normalize_code(code)
    if not normalized:
        return JSONResponse(status_code=400, content={"ok": False, "error": "invalid_code"})
    try:
        with _get_favorites_conn() as conn:
            conn.execute("INSERT OR IGNORE INTO favorites (code) VALUES (?)", (normalized,))
        return JSONResponse(content={"ok": True, "code": normalized})
    except Exception as exc:
        return JSONResponse(status_code=200, content={"ok": False, "error": f"favorite_add_failed:{exc}"})


@app.delete("/favorites/{code}")
@app.delete("/api/favorites/{code}")
def favorites_remove(code: str):
    normalized = _normalize_code(code)
    if not normalized:
        return JSONResponse(status_code=400, content={"ok": False, "error": "invalid_code"})
    try:
        with _get_favorites_conn() as conn:
            conn.execute("DELETE FROM favorites WHERE code = ?", (normalized,))
        return JSONResponse(content={"ok": True, "code": normalized})
    except Exception as exc:
        return JSONResponse(status_code=200, content={"ok": False, "error": f"favorite_remove_failed:{exc}"})


@app.get("/api/screener")
def screener():
    try:
        rows = _get_screener_rows()
        return JSONResponse(content={"items": rows, "errors": []})
    except Exception as exc:
        return JSONResponse(content={"items": [], "errors": [f"screener_failed:{exc}"]})


@app.post("/api/batch_bars")
def batch_bars(payload: dict = Body(default={})):  # { timeframe, codes, limit }
    timeframe = payload.get("timeframe", "monthly")
    codes = payload.get("codes", [])
    limit = min(int(payload.get("limit", 60)), 2000)

    if not codes:
        return JSONResponse(content={"timeframe": timeframe, "limit": limit, "items": {}})

    if timeframe == "daily":
        bars_table = "daily_bars"
        ma_table = "daily_ma"
        time_col = "date"
    else:
        bars_table = "monthly_bars"
        ma_table = "monthly_ma"
        time_col = "month"

    placeholders = ",".join(["?"] * len(codes))
    query = f"""
        WITH base AS (
            SELECT b.code,
                   b.{time_col} AS t,
                   b.o,
                   b.h,
                   b.l,
                   b.c,
                   m.ma7,
                   m.ma20,
                   m.ma60,
                   ROW_NUMBER() OVER (PARTITION BY b.code ORDER BY b.{time_col} DESC) AS rn
            FROM {bars_table} b
            LEFT JOIN {ma_table} m
              ON b.code = m.code AND b.{time_col} = m.{time_col}
            WHERE b.code IN ({placeholders})
        )
        SELECT code, t, o, h, l, c, ma7, ma20, ma60
        FROM base
        WHERE rn <= ?
        ORDER BY code, t
    """

    with get_conn() as conn:
        rows = conn.execute(query, codes + [limit]).fetchall()
        monthly_rows = conn.execute(
            f"""
            SELECT code, month, o, h, l, c
            FROM monthly_bars
            WHERE code IN ({placeholders})
            ORDER BY code, month
            """,
            codes
        ).fetchall()

    monthly_by_code: dict[str, list[tuple]] = {}
    for code, month, o, h, l, c in monthly_rows:
        monthly_by_code.setdefault(code, []).append((month, o, h, l, c))

    boxes_by_code = {code: detect_boxes(monthly_by_code.get(code, [])) for code in codes}

    items: dict[str, dict[str, list]] = {
        code: {"bars": [], "ma": {"ma7": [], "ma20": [], "ma60": []}, "boxes": boxes_by_code.get(code, [])}
        for code in codes
    }
    for code, t, o, h, l, c, ma7, ma20, ma60 in rows:
        payload = items.setdefault(code, {"bars": [], "ma": {"ma7": [], "ma20": [], "ma60": []}, "boxes": boxes_by_code.get(code, [])})
        payload["bars"].append([t, o, h, l, c])
        payload["ma"]["ma7"].append([t, ma7])
        payload["ma"]["ma20"].append([t, ma20])
        payload["ma"]["ma60"].append([t, ma60])

    return JSONResponse(content={"timeframe": timeframe, "limit": limit, "items": items})


@app.get("/api/ticker/daily")
def daily(code: str, limit: int = 400):
    query_with_ma = """
        WITH base AS (
            SELECT
                b.date,
                b.o,
                b.h,
                b.l,
                b.c,
                b.v,
                m.ma7,
                m.ma20,
                m.ma60
            FROM daily_bars b
            LEFT JOIN daily_ma m
              ON b.code = m.code AND b.date = m.date
            WHERE b.code = ?
            ORDER BY b.date
        ),
        tail AS (
            SELECT *
            FROM base
            ORDER BY date DESC
            LIMIT ?
        )
        SELECT date, o, h, l, c, v, ma7, ma20, ma60
        FROM tail
        ORDER BY date
    """
    query_basic = """
        WITH base AS (
            SELECT
                b.date,
                b.o,
                b.h,
                b.l,
                b.c,
                b.v
            FROM daily_bars b
            WHERE b.code = ?
            ORDER BY b.date
        ),
        tail AS (
            SELECT *
            FROM base
            ORDER BY date DESC
            LIMIT ?
        )
        SELECT date, o, h, l, c, v
        FROM tail
        ORDER BY date
    """
    errors: list[str] = []
    try:
        with get_conn() as conn:
            rows = conn.execute(query_with_ma, [code, limit]).fetchall()
        return JSONResponse(content={"data": rows, "errors": []})
    except Exception as exc:
        errors.append(f"daily_query_failed:{exc}")
        try:
            with get_conn() as conn:
                rows = conn.execute(query_basic, [code, limit]).fetchall()
            return JSONResponse(content={"data": rows, "errors": []})
        except Exception as fallback_exc:
            errors.append(f"daily_query_fallback_failed:{fallback_exc}")
            return JSONResponse(content={"data": [], "errors": errors})


@app.get("/api/practice/session")
def practice_session(session_id: str | None = None):
    if not session_id:
        return JSONResponse(content={"error": "session_id_required"}, status_code=400)
    with _get_practice_conn() as conn:
        row = conn.execute(
            """
            SELECT
                session_id,
                code,
                start_date,
                end_date,
                cursor_time,
                max_unlocked_time,
                lot_size,
                range_months,
                trades,
                notes,
                ui_state
            FROM practice_sessions
            WHERE session_id = ?
            """,
            [session_id]
        ).fetchone()
    if not row:
        return JSONResponse(content={"session": None})
    trades_raw = row["trades"] or "[]"
    try:
        trades = json.loads(trades_raw)
        if not isinstance(trades, list):
            trades = []
    except (TypeError, ValueError):
        trades = []
    ui_state_raw = row["ui_state"] or "{}"
    try:
        ui_state = json.loads(ui_state_raw)
        if not isinstance(ui_state, dict):
            ui_state = {}
    except (TypeError, ValueError):
        ui_state = {}
    return JSONResponse(
        content={
            "session": {
                "session_id": row["session_id"],
                "code": row["code"],
                "start_date": row["start_date"],
                "end_date": row["end_date"],
                "cursor_time": row["cursor_time"],
                "max_unlocked_time": row["max_unlocked_time"],
                "lot_size": row["lot_size"],
                "range_months": row["range_months"],
                "trades": trades,
                "notes": row["notes"] or "",
                "ui_state": ui_state
            }
        }
    )


@app.post("/api/practice/session")
def practice_session_upsert(payload: dict = Body(...)):
    session_id = payload.get("session_id")
    code = payload.get("code")
    if not session_id or not code:
        return JSONResponse(content={"error": "session_id_code_required"}, status_code=400)
    start_date = _format_practice_date(payload.get("start_date"))
    end_date = _format_practice_date(payload.get("end_date"))
    cursor_time = payload.get("cursor_time")
    max_unlocked_time = payload.get("max_unlocked_time")
    lot_size = payload.get("lot_size")
    range_months = payload.get("range_months")
    trades = payload.get("trades")
    if not isinstance(trades, list):
        trades = []
    notes = payload.get("notes")
    if notes is not None:
        notes = str(notes)
    ui_state = payload.get("ui_state")
    if ui_state is None:
        ui_state = {}
    if not isinstance(ui_state, dict):
        ui_state = {}
    trades_json = json.dumps(trades, ensure_ascii=True)
    ui_state_json = json.dumps(ui_state, ensure_ascii=True)
    with _get_practice_conn() as conn:
        conn.execute(
            """
            INSERT INTO practice_sessions (
                session_id,
                code,
                start_date,
                end_date,
                cursor_time,
                max_unlocked_time,
                lot_size,
                range_months,
                trades,
                notes,
                ui_state,
                updated_at
            )
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
            ON CONFLICT(session_id) DO UPDATE SET
                code = excluded.code,
                start_date = excluded.start_date,
                end_date = excluded.end_date,
                cursor_time = excluded.cursor_time,
                max_unlocked_time = excluded.max_unlocked_time,
                lot_size = excluded.lot_size,
                range_months = excluded.range_months,
                trades = excluded.trades,
                notes = excluded.notes,
                ui_state = excluded.ui_state,
                updated_at = CURRENT_TIMESTAMP
            """,
            [
                session_id,
                code,
                start_date,
                end_date,
                cursor_time,
                max_unlocked_time,
                lot_size,
                range_months,
                trades_json,
                notes,
                ui_state_json
            ]
        )
    return JSONResponse(
        content={
            "session_id": session_id,
            "code": code,
            "start_date": start_date
        }
    )


@app.delete("/api/practice/session")
def practice_session_delete(session_id: str | None = None):
    if not session_id:
        return JSONResponse(content={"error": "session_id_required"}, status_code=400)
    with _get_practice_conn() as conn:
        conn.execute(
            "DELETE FROM practice_sessions WHERE session_id = ?",
            [session_id]
        )
    return JSONResponse(content={"deleted": True})


@app.get("/api/practice/sessions")
def practice_sessions(code: str | None = None):
    query = """
        SELECT
            session_id,
            code,
            start_date,
            end_date,
            cursor_time,
            max_unlocked_time,
            lot_size,
            range_months,
            trades,
            notes,
            ui_state,
            created_at,
            updated_at
        FROM practice_sessions
        {where_clause}
        ORDER BY datetime(updated_at) DESC
    """
    params: list = []
    where_clause = ""
    if code:
        where_clause = "WHERE code = ?"
        params.append(code)
    with _get_practice_conn() as conn:
        rows = conn.execute(query.format(where_clause=where_clause), params).fetchall()
    sessions: list[dict] = []
    for row in rows:
        trades_raw = row["trades"] or "[]"
        try:
            trades = json.loads(trades_raw)
            if not isinstance(trades, list):
                trades = []
        except (TypeError, ValueError):
            trades = []
        ui_state_raw = row["ui_state"] or "{}"
        try:
            ui_state = json.loads(ui_state_raw)
            if not isinstance(ui_state, dict):
                ui_state = {}
        except (TypeError, ValueError):
            ui_state = {}
        sessions.append(
            {
                "session_id": row["session_id"],
                "code": row["code"],
                "start_date": row["start_date"],
                "end_date": row["end_date"],
                "cursor_time": row["cursor_time"],
                "max_unlocked_time": row["max_unlocked_time"],
                "lot_size": row["lot_size"],
                "range_months": row["range_months"],
                "trades": trades,
                "notes": row["notes"] or "",
                "ui_state": ui_state,
                "created_at": row["created_at"],
                "updated_at": row["updated_at"]
            }
        )
    return JSONResponse(content={"sessions": sessions})


@app.get("/api/practice/daily")
def practice_daily(code: str, limit: int = 400, session_id: str | None = None, start_date: str | None = None):
    query_with_ma = """
        WITH base AS (
            SELECT
                b.date,
                b.o,
                b.h,
                b.l,
                b.c,
                b.v,
                m.ma7,
                m.ma20,
                m.ma60
            FROM daily_bars b
            LEFT JOIN daily_ma m
              ON b.code = m.code AND b.date = m.date
            WHERE b.code = ?
            {date_filter}
            ORDER BY b.date
        ),
        tail AS (
            SELECT *
            FROM base
            ORDER BY date DESC
            LIMIT ?
        )
        SELECT date, o, h, l, c, v, ma7, ma20, ma60
        FROM tail
        ORDER BY date
    """
    query_basic = """
        WITH base AS (
            SELECT
                b.date,
                b.o,
                b.h,
                b.l,
                b.c,
                b.v
            FROM daily_bars b
            WHERE b.code = ?
            {date_filter}
            ORDER BY b.date
        ),
        tail AS (
            SELECT *
            FROM base
            ORDER BY date DESC
            LIMIT ?
        )
        SELECT date, o, h, l, c, v
        FROM tail
        ORDER BY date
    """
    errors: list[str] = []
    parsed_start = _parse_practice_date(start_date) if start_date is not None else None
    resolved = _resolve_practice_start_date(session_id, start_date)
    if start_date is not None and parsed_start is None and resolved is None:
        errors.append("practice_start_date_invalid")
    date_filter = ""
    params: list = [code]
    date_value = None
    try:
        with get_conn() as conn:
            if resolved:
                max_date = conn.execute(
                    "SELECT MAX(date) FROM daily_bars WHERE code = ?",
                    [code]
                ).fetchone()[0]
                use_epoch = max_date is not None and max_date >= 1_000_000_000
                if use_epoch:
                    date_value = int(calendar.timegm(resolved.timetuple()))
                else:
                    date_value = resolved.year * 10000 + resolved.month * 100 + resolved.day
                date_filter = "AND b.date <= ?"
                params.append(date_value)
            params.append(limit)
            rows = conn.execute(query_with_ma.format(date_filter=date_filter), params).fetchall()
        return JSONResponse(content={"data": rows, "errors": errors})
    except Exception as exc:
        errors.append(f"daily_query_failed:{exc}")
        try:
            with get_conn() as conn:
                fallback_params: list = [code]
                if resolved:
                    if date_value is None:
                        max_date = conn.execute(
                            "SELECT MAX(date) FROM daily_bars WHERE code = ?",
                            [code]
                        ).fetchone()[0]
                        use_epoch = max_date is not None and max_date >= 1_000_000_000
                        if use_epoch:
                            date_value = int(calendar.timegm(resolved.timetuple()))
                        else:
                            date_value = resolved.year * 10000 + resolved.month * 100 + resolved.day
                        date_filter = "AND b.date <= ?"
                    fallback_params.append(date_value)
                fallback_params.append(limit)
                rows = conn.execute(query_basic.format(date_filter=date_filter), fallback_params).fetchall()
            return JSONResponse(content={"data": rows, "errors": errors})
        except Exception as fallback_exc:
            errors.append(f"daily_query_fallback_failed:{fallback_exc}")
            return JSONResponse(content={"data": [], "errors": errors})


@app.get("/api/practice/monthly")
def practice_monthly(
    code: str,
    limit: int = 240,
    session_id: str | None = None,
    start_date: str | None = None
):
    errors: list[str] = []
    parsed_start = _parse_practice_date(start_date) if start_date is not None else None
    resolved = _resolve_practice_start_date(session_id, start_date)
    if start_date is not None and parsed_start is None and resolved is None:
        errors.append("practice_start_date_invalid")
    month_filter = ""
    params: list = [code]
    month_value = None
    try:
        with get_conn() as conn:
            if resolved:
                max_month = conn.execute(
                    "SELECT MAX(month) FROM monthly_bars WHERE code = ?",
                    [code]
                ).fetchone()[0]
                use_epoch = max_month is not None and max_month >= 1_000_000_000
                if use_epoch:
                    month_value = int(calendar.timegm(resolved.replace(day=1).timetuple()))
                else:
                    month_value = resolved.year * 100 + resolved.month
                month_filter = "AND month <= ?"
                params.append(month_value)
            params.append(limit)
            rows = conn.execute(
                f"""
                WITH base AS (
                    SELECT
                        month,
                        o,
                        h,
                        l,
                        c
                    FROM monthly_bars
                    WHERE code = ?
                    {month_filter}
                    ORDER BY month DESC
                    LIMIT ?
                )
                SELECT month, o, h, l, c
                FROM base
                ORDER BY month
                """,
                params
            ).fetchall()
        return JSONResponse(content={"data": rows, "errors": errors})
    except Exception as exc:
        errors.append(f"monthly_query_failed:{exc}")
        return JSONResponse(content={"data": [], "errors": errors})


@app.get("/api/ticker/monthly")
def monthly(code: str, limit: int = 240):
    try:
        with get_conn() as conn:
            rows = conn.execute(
                """
                WITH base AS (
                    SELECT
                        month,
                        o,
                        h,
                        l,
                        c
                    FROM monthly_bars
                    WHERE code = ?
                    ORDER BY month DESC
                    LIMIT ?
                )
                SELECT month, o, h, l, c
                FROM base
                ORDER BY month
                """,
                [code, limit]
            ).fetchall()

        return JSONResponse(content={"data": rows, "errors": []})
    except Exception as exc:
        return JSONResponse(content={"data": [], "errors": [f"monthly_query_failed:{exc}"]})


@app.get("/api/ticker/boxes")
def ticker_boxes(code: str):
    with get_conn() as conn:
        rows = conn.execute(
            """
            SELECT month, o, h, l, c
            FROM monthly_bars
            WHERE code = ?
            ORDER BY month
            """,
            [code]
        ).fetchall()

    return JSONResponse(content=detect_boxes(rows))
